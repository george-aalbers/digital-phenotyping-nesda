{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40865508-8366-4bf2-9815-b2253e2c6371",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "This iPython notebook contains all code required to rerun the analysis for the paper 'Smartphone-tracked behavioural markers of depression and anxiety: Digital phenotyping in the Netherlands Study of Depression and Anxiety'. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9f394a-d27e-42d8-8a82-ebe74baf4291",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf6e65e-54d8-41b0-9d10-f14e00957b15",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea0aad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import pingouin as pg\n",
    "\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "import rpy2.robjects as robjects\n",
    "\n",
    "from config import make_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9559bf3-9669-42d9-b00d-c044e94b645e",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e33526-da81-47f0-af79-88dc94022f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir, raw_dir, interim_dir, processed_dir, evaluation_dir, performance_dir, \\\n",
    "model_dir, figure_dir, descriptives_dir, features, digital_features, descriptives_dict, \\\n",
    "raw_data_paths = make_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18b06e8-2b90-4c62-b566-4b1a616750fd",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9707e0b0-ce1b-4901-aca6-e1ccb87c19c1",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c836df8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(raw_dir, raw_data_paths):\n",
    "    '''\n",
    "    This function reads individual raw data files as pd.DataFrames and returns a list of pd.DataFrames.\n",
    "    \n",
    "    Parameters:\n",
    "    raw_dir (str): The directory where raw data files are stored.\n",
    "    raw_data_paths (list): List of file paths to raw data files.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of DataFrames containing the raw data.\n",
    "    '''\n",
    "    \n",
    "    # Read each CSV file into a DataFrame, using ';' as the separator\n",
    "    df_list = [pd.read_csv(raw_dir + path, sep=';') for path in raw_data_paths]\n",
    "    \n",
    "    # Replace the values in the \"sex\" column for the third DataFrame (assume 0 for female and 1 for male)\n",
    "    df_list[2][\"sex\"] = df_list[2].sex.replace({2: 1, 1: 0})\n",
    "    \n",
    "    # Select only specific columns from the third DataFrame\n",
    "    df_list[2] = df_list[2][[\"pident\", \"sex\", \"gage\", \"gedu\"]]\n",
    "    \n",
    "    return df_list\n",
    "\n",
    "def merge_data(df_list):\n",
    "    '''\n",
    "    This function takes the list of pd.DataFrames as an input and returns a single merged pd.DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df_list (list): A list of DataFrames to be merged.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A single DataFrame merged on the \"pident\" column.\n",
    "    '''\n",
    "    \n",
    "    # Start with the first DataFrame in the list\n",
    "    df = df_list[0]\n",
    "    \n",
    "    # Merge the rest of the DataFrames on the \"pident\" column\n",
    "    for single_df in df_list[1:]:\n",
    "        df = pd.merge(df, single_df, on=\"pident\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def make_outcome_variables(df, ids_threshold=13, bai_threshold=9):\n",
    "    '''\n",
    "    This function takes the merged pd.DataFrame and creates three outcome variables: \"group\", \"group_dep\", and \"group_anx\". \n",
    "    \"group\" is used for the main analysis, while \"group_dep\" and \"group_anx\" are used for sensitivity analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The merged DataFrame.\n",
    "    ids_threshold (int): Threshold for the IDS score to define symptomatic individuals.\n",
    "    bai_threshold (int): Threshold for the BAI score to define symptomatic individuals.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with additional outcome variables.\n",
    "    '''\n",
    "\n",
    "    # Create a binary variable indicating whether the individual has a current depression disorder\n",
    "    df[\"dep_status\"] = df.gcidep10 > 0\n",
    "\n",
    "    # Create a binary variable indicating whether the individual has a current anxiety disorder\n",
    "    df[\"anx_status\"] = df.ganxy21 > 0\n",
    "\n",
    "    # Create a binary variable indicating whether the individual has either a depression or anxiety disorder\n",
    "    df[\"psychiatric_status\"] = 1 * ((df.dep_status + df.anx_status) > 0)\n",
    "\n",
    "    # Create a binary variable indicating whether the individual has any depression or anxiety symptoms\n",
    "    df[\"symptomatic\"] = 1 * (\n",
    "        (df.gids > ids_threshold) | (df.gbaiscal > bai_threshold)\n",
    "    )\n",
    "\n",
    "    # Create a categorical variable representing the individual's group: asymptomatic, subthreshold, or current diagnosis\n",
    "    df[\"group\"] = df.psychiatric_status.astype(str) + \"-\" + df.symptomatic.astype(str)\n",
    "    \n",
    "    # Replace the group codes with descriptive labels\n",
    "    df = df.replace(\n",
    "        {\n",
    "            \"0-0\": \"asymptomatic\",\n",
    "            \"0-1\": \"subthreshold\",\n",
    "            \"1-0\": \"symptomatic\",\n",
    "            \"1-1\": \"symptomatic\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create a binary variable indicating whether the individual has a depression diagnosis or symptoms but no anxiety issues\n",
    "    df['group_dep'] = 1 * (((df.dep_status > 0) | (df.gids > ids_threshold)) & ((df.anx_status == 0) & (df.gbaiscal <= bai_threshold))) \n",
    "\n",
    "    # Create a binary variable indicating whether the individual has an anxiety diagnosis or symptoms but no depression issues\n",
    "    df['group_anx'] = 1 * ((df.anx_status > 0) | (df.gbaiscal > bai_threshold)) & ((df.dep_status == 0) & (df.gids <= ids_threshold)) \n",
    " \n",
    "    return df\n",
    "\n",
    "def rename_columns(df):\n",
    "    '''\n",
    "    This function renames all columns from the raw NESDA data to a more human-readable format.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame with original column names.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with renamed columns.\n",
    "    '''\n",
    "    \n",
    "    # Load the dictionary for renaming columns from a JSON file\n",
    "    nesda_dict = pd.read_json(raw_dir + \"nesda_dictionary.json\", typ=\"series\")\n",
    "    \n",
    "    # Rename columns according to the dictionary\n",
    "    df = df.rename(nesda_dict, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def replace_spaces_by_nans(df):\n",
    "    '''\n",
    "    This function replaces string spaces in the data with numpy NaN values.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame with potential spaces as missing values.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with spaces replaced by NaN.\n",
    "    '''\n",
    "    \n",
    "    return df.replace(\" \", np.nan)\n",
    "\n",
    "def prepare_data(raw_dir, raw_data_paths):\n",
    "    '''\n",
    "    This function combines all functions required to prepare the NESDA data. Returns a merged pd.DataFrame\n",
    "    with human-readable columns for every feature and outcome (for both main and sensitivity analysis).\n",
    "    \n",
    "    Parameters:\n",
    "    raw_dir (str): The directory where raw data files are stored.\n",
    "    raw_data_paths (list): List of file paths to raw data files.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A prepared and cleaned DataFrame ready for analysis.\n",
    "    '''\n",
    "    \n",
    "    # Read the raw data into a list of DataFrames\n",
    "    df_list = read_data(raw_dir, raw_data_paths)\n",
    "\n",
    "    # Merge the DataFrames into a single DataFrame\n",
    "    df = merge_data(df_list)\n",
    "\n",
    "    # Create outcome variables\n",
    "    df = make_outcome_variables(df)\n",
    "\n",
    "    # Rename columns to be more descriptive\n",
    "    df = rename_columns(df)\n",
    "\n",
    "    # Replace spaces with NaNs for proper handling of missing data\n",
    "    df = replace_spaces_by_nans(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_pps_without_behapp(df):\n",
    "    '''\n",
    "    This function removes participants without any digital phenotyping data.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame with participant data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with participants without digital phenotyping data removed.\n",
    "    '''\n",
    "    \n",
    "    return df[df.has_data]\n",
    "\n",
    "def drop_pps_with_ios(df):\n",
    "    '''\n",
    "    This function removes participants who do not use an Android operating system.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame with participant data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with iOS participants removed.\n",
    "    '''\n",
    "    \n",
    "    return df[df.platform == \"android\"].drop(\"platform\", axis=1)\n",
    "\n",
    "def drop_pps_without_sufficient_days(df):\n",
    "    '''\n",
    "    This function drops participants with less than 7 days of location and app data.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame with participant data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with participants without sufficient data removed.\n",
    "    '''\n",
    "    \n",
    "    # Create a binary variable indicating sufficient data availability\n",
    "    df[\"sufficient_days\"] = (df.unique_number_of_days_with_location_data > 6) * (\n",
    "        df.unique_number_of_days_with_app_data > 6\n",
    "    )\n",
    "    \n",
    "    return df[df.sufficient_days]\n",
    "\n",
    "def select_columns(df):\n",
    "    '''\n",
    "    This function selects all relevant columns for further analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame with all columns.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame with only the relevant columns.\n",
    "    '''\n",
    "    \n",
    "    # Extract the participant ID\n",
    "    df_id = df.id\n",
    "\n",
    "    # Select demographic features\n",
    "    df_demographics = df[[\"demo_age\", \"demo_sex\", \"demo_edu\"]]\n",
    "\n",
    "    # Select app usage features\n",
    "    df_app = df.filter(like=\"app\")\n",
    "\n",
    "    # Select location data features\n",
    "    df_location = df.filter(like=\"location\")\n",
    "\n",
    "    # Select severity scores for depression and anxiety\n",
    "    df_severity = df[[\"ids_total_score\", \"bai_total_score\"]]\n",
    "\n",
    "    # Select outcome variables\n",
    "    df_status = df[[\"dep_status\", \"anx_status\", \"psychiatric_status\", \"group\", \"group_dep\", \"group_anx\"]]\n",
    "\n",
    "    # Concatenate all selected columns into one DataFrame\n",
    "    return pd.concat([df_id, df_demographics, df_app, df_location, df_severity, df_status], axis=1)\n",
    "\n",
    "def deal_with_subthreshold(df, approach):\n",
    "    '''\n",
    "    This function allows handling the subthreshold group by either excluding it, merging it with controls, or merging it with cases.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the group column.\n",
    "    approach (str): The approach for handling the subthreshold group: \"exclude\", \"merge-with-controls\", or \"merge-with-cases\".\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with the subthreshold group handled according to the specified approach.\n",
    "    '''\n",
    "    \n",
    "    if approach == \"exclude\":\n",
    "        # Exclude subthreshold group from the data\n",
    "        df = df[df.group != \"subthreshold\"]\n",
    "    elif approach == \"merge-with-controls\":\n",
    "        # Merge subthreshold group with asymptomatic controls\n",
    "        df = df.replace({\"subthreshold\": \"asymptomatic\"})\n",
    "    elif approach == \"merge-with-cases\":\n",
    "        # Merge subthreshold group with current cases\n",
    "        df = df.replace({\"subthreshold\": \"symptomatic\"})\n",
    "\n",
    "    # Update the psychiatric_status column based on the new group labels\n",
    "    df.psychiatric_status = df.group.replace({\"asymptomatic\": 0, \"symptomatic\": 1}).fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def make_interim_data(\n",
    "    raw_dir,\n",
    "    raw_data_paths,\n",
    "    subthreshold_approach=\"merge-with-cases\",\n",
    "    drop_ios=True,\n",
    "    drop_insufficient_days=True,\n",
    "    drop_columns=True\n",
    "):\n",
    "    '''\n",
    "    This function prepares an interim dataset that can be used for visualization and description. The dataset is processed \n",
    "    through other steps of the pipeline (imputation, scaling, and feature selection) before hyperparameter tuning and model evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "    raw_dir (str): The directory where raw data files are stored.\n",
    "    raw_data_paths (list): List of file paths to raw data files.\n",
    "    subthreshold_approach (str, optional): Approach for handling subthreshold cases (\"exclude\", \"merge-with-controls\", \"merge-with-cases\"). \n",
    "                                           Default is \"merge-with-cases\".\n",
    "    drop_ios (bool, optional): Whether to drop participants with iOS devices. Default is True.\n",
    "    drop_insufficient_days (bool, optional): Whether to drop participants with insufficient days of data. Default is True.\n",
    "    drop_columns (bool, optional): Whether to select and keep only relevant columns. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The prepared and cleaned interim dataset.\n",
    "    '''\n",
    "\n",
    "    # Prepare the data by reading, merging, and processing the raw files\n",
    "    df = prepare_data(raw_dir, raw_data_paths)\n",
    "\n",
    "    # Drop participants without any digital phenotyping data\n",
    "    df = drop_pps_without_behapp(df)\n",
    "\n",
    "    if drop_ios:\n",
    "        # Drop participants using iOS devices if specified\n",
    "        df = drop_pps_with_ios(df)\n",
    "\n",
    "    if drop_insufficient_days:\n",
    "        # Drop participants without sufficient data if specified\n",
    "        df = drop_pps_without_sufficient_days(df)\n",
    "\n",
    "    if drop_columns:\n",
    "        # Select only relevant columns for analysis if specified\n",
    "        df = select_columns(df)\n",
    "\n",
    "    if subthreshold_approach != \"\":\n",
    "        # Handle the subthreshold group according to the specified approach\n",
    "        df = deal_with_subthreshold(df, subthreshold_approach)\n",
    "\n",
    "    # Reset the index for the final DataFrame\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ee36c7-feaf-467c-9b0c-216d931484e9",
   "metadata": {},
   "source": [
    "# Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c30aa855-2447-4f41-9a74-067d56252866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "invalid value encountered in scalar divide\n",
      "invalid value encountered in scalar divide\n",
      "invalid value encountered in scalar divide\n",
      "divide by zero encountered in scalar divide\n",
      "Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "R[write to console]: \n",
      "Attaching package: ‘lubridate’\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    date, intersect, setdiff, union\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n",
      "✔ dplyr   1.1.4     ✔ readr   2.1.5\n",
      "✔ forcats 1.0.0     ✔ stringr 1.5.1\n",
      "✔ ggplot2 3.5.0     ✔ tibble  3.2.1\n",
      "✔ purrr   1.0.2     ✔ tidyr   1.3.1\n",
      "── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "✖ dplyr::filter() masks stats::filter()\n",
      "✖ dplyr::lag()    masks stats::lag()\n",
      "ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Picking joint bandwidth of 0.034\n",
      "\n",
      "R[write to console]: Picking joint bandwidth of 0.0251\n",
      "\n",
      "R[write to console]: In addition: \n",
      "R[write to console]: Warning messages:\n",
      "\n",
      "R[write to console]: 1: `fct_reorder()` removing 119 missing values.\n",
      "ℹ Use `.na_rm = TRUE` to silence this message.\n",
      "ℹ Use `.na_rm = FALSE` to preserve NAs. \n",
      "\n",
      "R[write to console]: 2: Removed 124 rows containing non-finite outside the scale range\n",
      "(`stat_density_ridges()`). \n",
      "\n",
      "Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_descriptives(raw_dir, raw_data_paths, grouping_var):\n",
    "    '''\n",
    "    This function prepares the data for creating a descriptive statistics table, grouped by a specified variable.\n",
    "\n",
    "    It performs the following steps:\n",
    "    1. Prepares the data by calling `prepare_data()` and `make_interim_data()` to handle subthreshold cases.\n",
    "    2. Filters the main dataset to include only the IDs present in the interim data.\n",
    "    3. Replaces specific values (-3.0, -2.0, -1.0) with NaN to clean the data.\n",
    "    4. Groups the data by the specified grouping variable and aggregates using predefined statistics.\n",
    "    5. Transforms the DataFrame to have variables as rows and statistics as columns.\n",
    "    6. Performs various string replacements to make the variable names more readable.\n",
    "    7. Returns the final DataFrame, which can be used for generating descriptive tables.\n",
    "\n",
    "    Parameters:\n",
    "    - raw_dir (str): The directory containing the raw data.\n",
    "    - raw_data_paths (list): List of paths to the raw data files.\n",
    "    - grouping_var (str): The variable by which to group the data (e.g., 'group').\n",
    "\n",
    "    Returns:\n",
    "    - df (DataFrame): A DataFrame containing the descriptive statistics.\n",
    "    '''\n",
    "    df = prepare_data(raw_dir, raw_data_paths)\n",
    "    df_ = make_interim_data(raw_dir, raw_data_paths, subthreshold_approach=\"merge-with-cases\")\n",
    "    df = df[df.id.isin(df_.id)]\n",
    "    df = deal_with_subthreshold(df, approach=\"merge-with-cases\")\n",
    "    subgroup_count = df.group.value_counts()\n",
    "    df = df.replace({-3.0: np.nan, -2.0: np.nan, -1.0: np.nan})\n",
    "    df = df.groupby(grouping_var).aggregate(descriptives_dict)\n",
    "    df = df.T\n",
    "    df = df.reset_index()\n",
    "    df = df.rename({\"level_0\": \"variable\", \"level_1\": \"statistic\"}, axis=1)\n",
    "    df = df.pivot(index=\"variable\", columns=\"statistic\")\n",
    "    df = df.fillna(\"-\")\n",
    "    df.index = df.index.str.replace(\"_\", \" \").str.capitalize()\n",
    "    df.index = df.index.str.replace(\"month\", \"diagnosis in the past month\")\n",
    "    df.index = df.index.str.replace(\"6 diagnosis in the past month\", \"diagnosis in the past 6 months\")\n",
    "    df.index = df.index.str.replace(\"since last interview\", \"diagnosis in the between W7 and last interview\")\n",
    "    df.index = df.index.str.replace(\"year\", \"diagnosis in the past year\")\n",
    "    df.index = df.index.str.replace(\"N\", \"Number of\")\n",
    "    df.index = df.index.str.replace(\"App \", \"App feature - \")\n",
    "    df.index = df.index.str.replace(\"Call \", \"Call feature - \")\n",
    "    df.index = df.index.str.replace(\"Location \", \"Location feature - \")\n",
    "    df.index = df.index.str.replace(\"Number ofumber\", \"Number\")\n",
    "    df.index = df.index.str.replace(\"Bai\", \"BAI\")\n",
    "    df.index = df.index.str.replace(\"Mdd\", \"MDD\")\n",
    "    df.index = df.index.str.replace(\"mdd\", \"MDD\")\n",
    "    df.index = df.index.str.replace(\"Ids\", \"IDS\")\n",
    "    df.index = df.index.str.replace(\"Qids\", \"QIDS\")\n",
    "    df.index = df.index.str.replace(\"Gad\", \"GAD\")\n",
    "    df.index = df.index.str.replace(\"Dys\", \"Dysthymia\")\n",
    "    return df\n",
    "\n",
    "def make_feature_descriptives(raw_dir, descriptives_dir):\n",
    "    '''\n",
    "    This function generates a descriptive statistics table for all digital phenotyping features.\n",
    "\n",
    "    It performs the following steps:\n",
    "    1. Calls `prepare_descriptives()` to prepare the data for creating the descriptives table.\n",
    "    2. Filters the data to include only rows related to digital phenotyping features.\n",
    "    3. Splits the variable names into feature modality and feature name for clarity.\n",
    "    4. Sets the index to a combination of feature modality and feature name.\n",
    "    5. Reorders the columns to include median, mean, standard deviation, minimum, and maximum.\n",
    "    6. Saves the descriptive statistics to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - raw_dir (str): The directory containing the raw data.\n",
    "    - descriptives_dir (str): The directory where the descriptive statistics file will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function saves the descriptive statistics to a CSV file.\n",
    "    '''\n",
    "    df = prepare_descriptives(raw_dir, raw_data_paths, grouping_var=\"group\")\n",
    "    df = df.filter(like=\"feature\", axis=0).reset_index()\n",
    "    df[[\"feature modality\", \"feature name\"]] = df.variable.str.split(\" - \", expand=True)\n",
    "    df[\"feature name\"] = df[\"feature name\"].str.capitalize()\n",
    "    df = df.set_index([\"feature modality\", \"feature name\"])\n",
    "    df = df.reindex(columns=[\"median\", \"mean\", \"std\", \"min\", \"max\"], level=\"statistic\")\n",
    "    df.to_csv(descriptives_dir + \"descriptives-features.csv\")\n",
    "\n",
    "def make_diagnostic_descriptives(raw_dir, descriptives_dir):\n",
    "    '''\n",
    "    This function generates a descriptive statistics table for diagnostic information.\n",
    "\n",
    "    It performs the following steps:\n",
    "    1. Calls `prepare_descriptives()` to prepare the data for creating the diagnostic descriptives table.\n",
    "    2. Filters and reindexes the data for different diagnostic time periods (month, 6 months, year, since last interview).\n",
    "    3. Combines the filtered data into a single DataFrame.\n",
    "    4. Cleans up and formats the variable names for clarity.\n",
    "    5. Saves the descriptive statistics to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - raw_dir (str): The directory containing the raw data.\n",
    "    - descriptives_dir (str): The directory where the diagnostic descriptives file will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function saves the diagnostic descriptive statistics to a CSV file.\n",
    "    '''\n",
    "    df = prepare_descriptives(raw_dir, raw_data_paths, grouping_var=\"group\")\n",
    "    df_month = df.filter(like=\"diagnosis in the past month\", axis=0).reindex(columns=[\"sum\"], level=\"statistic\")\n",
    "    df_6_month = df.filter(like=\"diagnosis in the past 6 months\", axis=0).reindex(columns=[\"sum\"], level=\"statistic\")\n",
    "    df_6_month = df_6_month[~df_6_month.index.isin([\"Number of anxiety disorders diagnosis in the past 6 months\", \"Number of diagnoses diagnosis in the past 6 months\"])]\n",
    "    df_year = df.filter(like=\"diagnosis in the past year\", axis=0).reindex(columns=[\"sum\"], level=\"statistic\")\n",
    "    df_last_interview = df.filter(like=\"diagnosis in the between W7 and last interview\", axis=0).reindex(columns=[\"sum\"], level=\"statistic\")\n",
    "    df = pd.concat([df_month, df_6_month, df_year, df_last_interview], axis=0)\n",
    "    df = df.astype(int)\n",
    "    df = df.reset_index()\n",
    "    df[[\"Diagnosis\", \"Time period\"]] = df.variable.str.split(\" diagnosis in the \", expand=True)\n",
    "    df = df.droplevel(level=1, axis=1)\n",
    "    df = df[[\"Time period\", \"Diagnosis\", \"asymptomatic\", \"symptomatic\"]]\n",
    "    df = df.set_index([\"Time period\", \"Diagnosis\"])\n",
    "    df.to_csv(descriptives_dir + \"descriptives-diagnostic.csv\")\n",
    "\n",
    "def make_self_report_descriptives(raw_dir, descriptives_dir):\n",
    "    '''\n",
    "    This function generates a descriptive statistics table for clinical self-reports.\n",
    "\n",
    "    It performs the following steps:\n",
    "    1. Calls `prepare_descriptives()` to prepare the data for creating the self-report descriptives table.\n",
    "    2. Filters the data to include only rows related to clinical self-reports (e.g., IDS, BAI).\n",
    "    3. Reorders the columns to include median, mean, standard deviation, minimum, and maximum.\n",
    "    4. Saves the descriptive statistics to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - raw_dir (str): The directory containing the raw data.\n",
    "    - descriptives_dir (str): The directory where the self-report descriptives file will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function saves the self-report descriptive statistics to a CSV file.\n",
    "    '''\n",
    "    df = prepare_descriptives(raw_dir, raw_data_paths, grouping_var=\"group\")\n",
    "    df = pd.concat([df.filter(like=\"IDS\", axis=0), df.filter(like=\"BAI\", axis=0)], axis=0)\n",
    "    df = df.reindex(columns=[\"median\", \"mean\", \"std\", \"min\", \"max\"], level=\"statistic\")\n",
    "    df.to_csv(descriptives_dir + \"descriptives-selfreport.csv\")\n",
    "\n",
    "def make_demo_descriptives(raw_dir, descriptives_dir, raw_data_paths):\n",
    "    '''\n",
    "    This function generates a descriptive statistics table for demographic information.\n",
    "\n",
    "    It performs the following steps:\n",
    "    1. Prepares the data by calling `prepare_data()` and `make_interim_data()` to handle subthreshold cases.\n",
    "    2. Filters the main dataset to include only the IDs present in the interim data.\n",
    "    3. Deals with subthreshold cases by either merging them with other groups or excluding them.\n",
    "    4. Replaces the numeric encoding of sex with human-readable labels (e.g., \"Male\", \"Female\").\n",
    "    5. Aggregates the data by group to calculate counts and descriptive statistics for sex, education, and age.\n",
    "    6. Combines the results into a single DataFrame and renames the columns appropriately.\n",
    "    7. Saves the descriptive statistics to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - raw_dir (str): The directory containing the raw data.\n",
    "    - descriptives_dir (str): The directory where the demographic descriptives file will be saved.\n",
    "    - raw_data_paths (list): List of paths to the raw data files.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function saves the demographic descriptive statistics to a CSV file.\n",
    "    '''\n",
    "    df = prepare_data(raw_dir, raw_data_paths)\n",
    "    df_ = make_interim_data(raw_dir, raw_data_paths, subthreshold_approach=\"merge-with-cases\")\n",
    "    df = df[df.id.isin(df_.id)]\n",
    "    df = deal_with_subthreshold(df, approach=\"merge-with-cases\")\n",
    "    df.demo_sex = df.demo_sex.replace({0: \"Male\", 1: \"Female\"})\n",
    "    \n",
    "    # Aggregating the data by group for sex, education, and age\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            df.groupby(\"group\").demo_sex.value_counts(),\n",
    "            df.groupby(\"group\").demo_edu.aggregate([\"mean\", \"std\"]),\n",
    "            df.groupby(\"group\").demo_age.aggregate([\"mean\", \"std\"]),\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "    \n",
    "    df.columns = [\"n\", \"mean\", \"std\"]\n",
    "    df.to_csv(descriptives_dir + \"descriptives-demo.csv\")\n",
    "\n",
    "def make_top_feature_desciptives():\n",
    "    '''\n",
    "    This function computes and ranks the effect sizes of group differences for each digital phenotyping feature.\n",
    "    It also calculates the mean, median, and standard deviation for these features within the groups.\n",
    "\n",
    "    It performs the following steps:\n",
    "    1. Prepares the data using `make_interim_data()` with subthreshold cases merged into the \"current\" group.\n",
    "    2. Iterates through each feature and calculates the Cohen's d effect size for differences between groups.\n",
    "    3. Collects the computed effect sizes and corresponding descriptive statistics into a DataFrame.\n",
    "    4. Filters and ranks the top features based on the magnitude of the effect size.\n",
    "    5. Formats the feature names for better readability and saves the top features to a CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function saves the descriptive statistics of the top features to a CSV file.\n",
    "    '''\n",
    "    df_ = make_interim_data(raw_dir, raw_data_paths, subthreshold_approach=\"merge-with-cases\")\n",
    "    effect_sizes = pd.DataFrame()\n",
    "\n",
    "    # Iterating over each feature and calculating effect sizes between groups\n",
    "    for var in df_.columns[6:]:\n",
    "        for group1 in [\"asymptomatic\", \"symptomatic\"]:\n",
    "            for group2 in [\"asymptomatic\", \"symptomatic\"]:\n",
    "                try:\n",
    "                    cohens_d = pg.compute_effsize(\n",
    "                        df_[var][df_.group == group1].dropna(),\n",
    "                        df_[var][df_.group == group2].dropna(),\n",
    "                    )\n",
    "                    effect_sizes = pd.concat(\n",
    "                        [\n",
    "                            effect_sizes,\n",
    "                            pd.DataFrame(\n",
    "                                {\n",
    "                                    \"feature\": var,\n",
    "                                    \"group1\": group1,\n",
    "                                    \"group2\": group2,\n",
    "                                    \"mean1\": df_[var][df_.group == group1].mean(),\n",
    "                                    \"median1\": df_[var][df_.group == group1].median(),\n",
    "                                    \"sd1\": df_[var][df_.group == group1].std(),\n",
    "                                    \"mean2\": df_[var][df_.group == group2].mean(),\n",
    "                                    \"median2\": df_[var][df_.group == group2].median(),\n",
    "                                    \"sd2\": df_[var][df_.group == group2].std(),\n",
    "                                    \"d\": cohens_d,\n",
    "                                },\n",
    "                                index=[0],\n",
    "                            ),\n",
    "                        ]\n",
    "                    )\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    # Cleaning and sorting the effect sizes\n",
    "    effect_sizes = effect_sizes.replace({np.inf: np.nan, -np.inf: np.nan}).dropna()\n",
    "    effect_sizes[\"d\"] = effect_sizes.d.abs()\n",
    "\n",
    "    # Filtering for app and location features, and selecting the top features\n",
    "    app_es = effect_sizes.set_index(\"feature\").filter(like=\"app_\", axis=0)\n",
    "    loc_es = effect_sizes.set_index(\"feature\").filter(like=\"location_\", axis=0)\n",
    "    app_es = app_es.sort_values(by=\"d\", ascending=False).iloc[:20, :].round(2)\n",
    "    loc_es = loc_es.sort_values(by=\"d\", ascending=False).iloc[:20, :].round(2)\n",
    "\n",
    "    effect_size_df = pd.concat([app_es, loc_es], axis=0).reset_index()\n",
    "    effect_size_df.feature = (\n",
    "        effect_size_df.feature.str.replace(\"_\", \" \")\n",
    "        .str.split(\" \")\n",
    "        .apply(lambda x: x[1:])\n",
    "        .str.join(\" \")\n",
    "        .str.capitalize()\n",
    "    )\n",
    "    effect_size_df[effect_size_df.group1 == \"asymptomatic\"].to_csv(\n",
    "        descriptives_dir + \"descriptives-top-features.csv\"\n",
    "    )\n",
    "\n",
    "def make_ridgelines_plot(raw_dir, raw_data_paths, digital_features):\n",
    "    '''\n",
    "    This function prepares the data and generates a ridgeline plot to visualize the distribution of digital phenotyping features.\n",
    "\n",
    "    It performs the following steps:\n",
    "    1. Prepares the data using `make_interim_data()` and fills missing values for app and location features.\n",
    "    2. Scales the digital phenotyping features using MinMaxScaler.\n",
    "    3. Reshapes the data into a long format suitable for plotting.\n",
    "    4. Adds labels and groups for the features, reorders the variables based on their mean value.\n",
    "    5. Saves the prepared data to a CSV file and calls an external R script to generate the ridgeline plot.\n",
    "\n",
    "    Parameters:\n",
    "    - raw_dir (str): The directory containing the raw data.\n",
    "    - raw_data_paths (list): List of paths to the raw data files.\n",
    "    - digital_features (list): List of digital phenotyping features to include in the plot.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function saves the prepared data to a CSV file and generates the plot using an R script.\n",
    "    '''\n",
    "    df = make_interim_data(raw_dir, raw_data_paths)\n",
    "\n",
    "    # Filling missing values for app and location features\n",
    "    df[df.filter(like='app').columns] = df.filter(like='app').fillna(0)\n",
    "    df.location_total_number_of_nightly_staypoints_excluding_home = df.location_total_number_of_nightly_staypoints_excluding_home.fillna(0)\n",
    "  \n",
    "    # Scaling the digital features\n",
    "    scaler = MinMaxScaler()\n",
    "    df[digital_features] = scaler.fit_transform(df[digital_features])\n",
    "\n",
    "    # Reshaping the data for plotting\n",
    "    digital_features.append('group')\n",
    "    df = df[digital_features].melt(id_vars='group')\n",
    "    df.loc[:,'feature_group'] = df.variable.str.split('_').str[0]\n",
    "    df.group = df.group.replace({'symptomatic':'Symptomatic','asymptomatic':'Asymptomatic'})\n",
    "    df.feature_group = df.feature_group.replace({'app':'Smartphone app use','location':'Location'})\n",
    "    df.variable = df.variable.str.split('_').str[1:].str.join(' ').str.capitalize().replace('minutes','(minutes)',regex=True).replace('hours','(hours)',regex=True).replace('kilometers','(kilometers)',regex=True)\n",
    "\n",
    "    # Reordering the variables based on their mean value\n",
    "    df = pd.merge(df, df.groupby('variable').value.mean().sort_values(ascending=False), on='variable', suffixes=('_feature','_mean'))\n",
    "    df = df.sort_values(by='value_mean', ascending=False)\n",
    "    df.to_csv('ridgelines-df.csv', index=False)\n",
    "\n",
    "    # Calling an R script to generate the ridgeline plot\n",
    "    robjects.r('source(\"ridgelines-data-visualisation.R\")')\n",
    "\n",
    "def make_missingness_barplot():\n",
    "    '''\n",
    "    This function generates and saves a bar plot to visualize the number of missing values per feature.\n",
    "\n",
    "    It performs the following steps:\n",
    "    1. Prepares the data using `make_interim_data()` and fills missing values for app and location features.\n",
    "    2. Calculates the number of missing values per feature and formats the feature names.\n",
    "    3. Generates a horizontal bar plot showing the number of missing values for each feature.\n",
    "    4. Saves the plot to a PNG file.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function saves the missingness bar plot to a PNG file.\n",
    "    '''\n",
    "    df = make_interim_data(raw_dir, raw_data_paths, subthreshold_approach='merge-with-cases')\n",
    "\n",
    "    # Filling missing values for app and location features\n",
    "    df[df.filter(like='app').columns] = df.filter(like='app').fillna(0)\n",
    "    df.location_total_number_of_nightly_staypoints_excluding_home = df.location_total_number_of_nightly_staypoints_excluding_home.fillna(0)\n",
    "    \n",
    "    # Calculating the number of missing values per feature\n",
    "    df = df[digital_features].isna().sum()[df.isna().sum() != 0].reset_index()\n",
    "    \n",
    "    # Formatting the feature names\n",
    "    df.columns = ['Feature', 'Number of missing values']\n",
    "    df.Feature = df.Feature.str.capitalize().str.split('_').str.join(' ').str.replace('minutes', '(minutes)').str.replace('kilometers', '(kilometers)').str.replace('hours','(hours)')\n",
    "    \n",
    "    # Generating the bar plot\n",
    "    sns.barplot(data=df, x='Number of missing values', y='Feature', color='black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Saving the plot to a file\n",
    "    plt.savefig(figure_dir + 'missingness.png')\n",
    "    plt.clf()\n",
    "\n",
    "def run_all_descriptives_functions(raw_dir, raw_data_paths, descriptives_dir, digital_features):\n",
    "    '''\n",
    "    This function orchestrates the generation of all descriptives tables and figures by calling the relevant functions.\n",
    "\n",
    "    It performs the following steps:\n",
    "    1. Calls `make_demo_descriptives()` to generate demographic descriptives.\n",
    "    2. Calls `make_diagnostic_descriptives()` to generate descriptives for diagnostic information.\n",
    "    3. Calls `make_self_report_descriptives()` to generate descriptives for clinical self-reports.\n",
    "    4. Calls `make_feature_descriptives()` to generate descriptives for digital phenotyping features.\n",
    "    5. Calls `make_top_feature_desciptives()` to compute effect sizes and rank the top features.\n",
    "    6. Calls `make_ridgelines_plot()` to prepare data and generate ridgeline plots.\n",
    "    7. Calls `make_missingness_barplot()` to generate a bar plot of missing values.\n",
    "\n",
    "    Parameters:\n",
    "    - raw_dir (str): The directory containing the raw data.\n",
    "    - raw_data_paths (list): List of paths to the raw data files.\n",
    "    - descriptives_dir (str): The directory where the descriptive statistics files will be saved.\n",
    "    - digital_features (list): List of digital phenotyping features to include in the plots.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function orchestrates the generation and saving of various descriptives outputs.\n",
    "    '''\n",
    "    # Generating all descriptives tables and figures\n",
    "    make_demo_descriptives(raw_dir, descriptives_dir, raw_data_paths)\n",
    "    make_diagnostic_descriptives(raw_dir, descriptives_dir)\n",
    "    make_self_report_descriptives(raw_dir, descriptives_dir)\n",
    "    make_feature_descriptives(raw_dir, descriptives_dir)\n",
    "    make_top_feature_desciptives()\n",
    "    make_ridgelines_plot(raw_dir, raw_data_paths, digital_features)\n",
    "    make_missingness_barplot()\n",
    "\n",
    "run_all_descriptives_functions(raw_dir, raw_data_paths, descriptives_dir, digital_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906e9ed3-e519-435f-86dc-35627a55507b",
   "metadata": {},
   "source": [
    "# Make dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12827dc9-dc01-45e1-92e7-ef9dc103e3ea",
   "metadata": {},
   "source": [
    "## Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c1048c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cols(df, target, features):\n",
    "    '''\n",
    "    This function splits a DataFrame into three separate DataFrames: one for the target variable (y), one for the features (X),\n",
    "    and one for the IDs.\n",
    "\n",
    "    It performs the following steps:\n",
    "    1. Extracts the target column (y) from the DataFrame.\n",
    "    2. Extracts the specified feature columns (X) from the DataFrame.\n",
    "    3. Extracts the \"id\" column from the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame containing all the data.\n",
    "    - target (str): The name of the target column.\n",
    "    - features (list): A list of column names to be used as features.\n",
    "\n",
    "    Returns:\n",
    "    - X (DataFrame): The DataFrame containing the selected features.\n",
    "    - y (Series): The Series containing the target variable.\n",
    "    - id (Series): The Series containing the ID values.\n",
    "    '''\n",
    "    y = df[target]\n",
    "    X = df[features]\n",
    "    id = df[\"id\"]\n",
    "    return X, y, id\n",
    "\n",
    "\n",
    "def write_to_file(df, name):\n",
    "    '''\n",
    "    This function writes a DataFrame to a CSV file without including the index.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame to be written to a file.\n",
    "    - name (str): The path and filename where the DataFrame should be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function writes the DataFrame to a CSV file.\n",
    "    '''\n",
    "    df.to_csv(name, index=False)\n",
    "\n",
    "\n",
    "def write_all_to_file(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    id_train,\n",
    "    id_test,\n",
    "    interim_dir,\n",
    "    processed_dir,\n",
    "    fold,\n",
    "):\n",
    "    '''\n",
    "    This function writes multiple DataFrames (training and testing data for features, target, and IDs) to corresponding CSV files.\n",
    "\n",
    "    It performs the following steps:\n",
    "    1. Writes the training features, target, and IDs to the interim and processed directories.\n",
    "    2. Writes the testing features, target, and IDs to the interim and processed directories.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (DataFrame): The training features.\n",
    "    - X_test (DataFrame): The testing features.\n",
    "    - y_train (Series): The training target.\n",
    "    - y_test (Series): The testing target.\n",
    "    - id_train (Series): The training IDs.\n",
    "    - id_test (Series): The testing IDs.\n",
    "    - interim_dir (str): The directory where interim training and testing data will be saved.\n",
    "    - processed_dir (str): The directory where processed training and testing IDs will be saved.\n",
    "    - fold (int): The fold number for cross-validation.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function writes all the DataFrames to their respective CSV files.\n",
    "    '''\n",
    "    write_to_file(X_train, interim_dir + \"X_train_\" + str(fold) + \".csv\")\n",
    "    write_to_file(y_train, interim_dir + \"y_train_\" + str(fold) + \".csv\")\n",
    "    write_to_file(id_train, processed_dir + \"id_train_\" + str(fold) + \".csv\")\n",
    "    write_to_file(X_test, interim_dir + \"X_test_\" + str(fold) + \".csv\")\n",
    "    write_to_file(y_test, interim_dir + \"y_test_\" + str(fold) + \".csv\")\n",
    "    write_to_file(id_test, processed_dir + \"id_test_\" + str(fold) + \".csv\")\n",
    "\n",
    "def get_features_df(\n",
    "    raw_dir, interim_dir, processed_dir, features, target, subthreshold_approach\n",
    "):\n",
    "    '''\n",
    "    This function prepares and returns a DataFrame containing the selected features for further analysis.\n",
    "\n",
    "    It performs the following steps:\n",
    "    1. Prepares the data using `make_interim_data()` with the specified subthreshold approach.\n",
    "    2. Splits the DataFrame into features (X), target (y), and IDs using the `split_cols()` function.\n",
    "\n",
    "    Parameters:\n",
    "    - raw_dir (str): The directory containing the raw data.\n",
    "    - interim_dir (str): The directory where interim data is stored.\n",
    "    - processed_dir (str): The directory where processed data is stored.\n",
    "    - features (list): A list of column names to be used as features.\n",
    "    - target (str): The name of the target column.\n",
    "    - subthreshold_approach (str): The approach for handling subthreshold cases.\n",
    "\n",
    "    Returns:\n",
    "    - X (DataFrame): A DataFrame containing the selected features.\n",
    "    '''\n",
    "    df = make_interim_data(raw_dir, subthreshold_approach=subthreshold_approach)\n",
    "    X, y, id = split_cols(df, target, features)\n",
    "    return X\n",
    "\n",
    "def make_train_test_splits(\n",
    "    raw_dir, interim_dir, processed_dir, features, target, subthreshold_approach\n",
    "):\n",
    "    '''\n",
    "    This function generates and saves 5 stratified train-test splits for cross-validation.\n",
    "\n",
    "    It performs the following steps:\n",
    "    1. Prepares the data using `make_interim_data()` and splits it into features (X), target (y), and IDs.\n",
    "    2. Uses StratifiedKFold to split the data into 5 train-test pairs, ensuring stratification based on the target variable.\n",
    "    3. For each fold, it writes the training and testing data (features, target, IDs) to the appropriate directories.\n",
    "\n",
    "    Parameters:\n",
    "    - raw_dir (str): The directory containing the raw data.\n",
    "    - interim_dir (str): The directory where interim data is stored.\n",
    "    - processed_dir (str): The directory where processed data is stored.\n",
    "    - features (list): A list of column names to be used as features.\n",
    "    - target (str): The name of the target column.\n",
    "    - subthreshold_approach (str): The approach for handling subthreshold cases.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function saves the train-test splits to the specified directories.\n",
    "    '''\n",
    "    df = make_interim_data(raw_dir, raw_data_paths, subthreshold_approach=subthreshold_approach)\n",
    "\n",
    "    X, y, id = split_cols(df, target, features)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        X_train = X.loc[train_index, :]\n",
    "        X_test = X.loc[test_index, :]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "        id_train = id[train_index]\n",
    "        id_test = id[test_index]\n",
    "\n",
    "        write_all_to_file(\n",
    "            X_train,\n",
    "            X_test,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            id_train,\n",
    "            id_test,\n",
    "            interim_dir,\n",
    "            processed_dir,\n",
    "            fold\n",
    "        )\n",
    "\n",
    "\n",
    "args = pd.read_json(\"config.json\")\n",
    "\n",
    "make_train_test_splits(raw_dir, interim_dir, processed_dir, **args.loc['main', :].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd32e2c-7ec7-4010-bb00-d183daebb599",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8785b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(train_file, test_file):\n",
    "    '''Helper function to read multiple files at once.\n",
    "    \n",
    "    This function reads training and testing datasets from CSV files.\n",
    "\n",
    "    It performs the following steps:\n",
    "    1. Reads the training data from the specified file path.\n",
    "    2. Reads the testing data from the specified file path.\n",
    "    3. Returns both datasets as pandas DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - train_file (str): Path to the training data CSV file.\n",
    "    - test_file (str): Path to the testing data CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - X_train (DataFrame): The training dataset.\n",
    "    - X_test (DataFrame): The testing dataset.\n",
    "    '''\n",
    "    X_train = pd.read_csv(train_file)\n",
    "    X_test = pd.read_csv(test_file)\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def write_to_file(df, name):\n",
    "    '''Helper function to write a DataFrame to a CSV file.\n",
    "    \n",
    "    This function saves a pandas DataFrame to a specified file without including the index.\n",
    "\n",
    "    It performs the following steps:\n",
    "    1. Saves the DataFrame to a CSV file at the specified location.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame to be saved.\n",
    "    - name (str): The file path and name where the DataFrame should be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function writes the DataFrame to a file.\n",
    "    '''\n",
    "    df.to_csv(name, index=False)\n",
    "\n",
    "\n",
    "def read_features(dir, fold):\n",
    "    '''Helper function to read train and test features for a single fold at once.\n",
    "    \n",
    "    This function reads the training and testing datasets for a specific cross-validation fold from CSV files.\n",
    "\n",
    "    It performs the following steps:\n",
    "    1. Constructs the file paths for the training and testing datasets based on the specified fold number.\n",
    "    2. Reads the datasets using the `read_data` helper function.\n",
    "    3. Returns the training and testing datasets along with their file paths.\n",
    "\n",
    "    Parameters:\n",
    "    - dir (str): Directory where the interim data is stored.\n",
    "    - fold (int): The fold number (e.g., 0, 1, 2, 3, or 4).\n",
    "\n",
    "    Returns:\n",
    "    - X_train (DataFrame): The training dataset for the specified fold.\n",
    "    - X_test (DataFrame): The testing dataset for the specified fold.\n",
    "    - X_train_name (str): The file path of the training dataset.\n",
    "    - X_test_name (str): The file path of the testing dataset.\n",
    "    '''\n",
    "    X_train_name = interim_dir + \"X_train_\" + str(fold) + \".csv\"\n",
    "    X_test_name = interim_dir + \"X_test_\" + str(fold) + \".csv\"\n",
    "    X_train, X_test = read_data(X_train_name, X_test_name)\n",
    "    return X_train, X_test, X_train_name, X_test_name\n",
    "\n",
    "\n",
    "def feature_imputation(interim_dir, skip=False):\n",
    "    '''Function imputes missing values in the train and test datasets. \n",
    "    \n",
    "    This function performs imputation on missing values in the training and testing datasets for multiple folds.\n",
    "    Features that should be zero when missing are imputed with zero. Mean imputation can be optionally skipped\n",
    "    for experimentation with other imputation approaches.\n",
    "\n",
    "    It performs the following steps:\n",
    "    1. Iterates over each cross-validation fold (0 to 4).\n",
    "    2. Reads the training and testing datasets for the fold using the `read_features` helper function.\n",
    "    3. Drops columns that have all missing values.\n",
    "    4. Imputes missing values in app-related features and location staypoints with zero.\n",
    "    5. If `skip` is False, imputes remaining missing values with the mean of the respective feature in the training dataset.\n",
    "    6. Saves the imputed datasets back to their respective files.\n",
    "\n",
    "    Parameters:\n",
    "    - interim_dir (str): Directory where the interim data is stored.\n",
    "    - skip (bool): If True, skips mean imputation. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function saves the imputed datasets to the specified file paths.\n",
    "    '''\n",
    "    for fold in range(5):\n",
    "\n",
    "        X_train, X_test, X_train_name, X_test_name = read_features(interim_dir, fold)\n",
    "\n",
    "        # Drop columns that have all missing values\n",
    "        X_train = X_train.dropna(how=\"all\", axis=1)\n",
    "\n",
    "        # Impute app-related features and location staypoints with zero\n",
    "        X_train[X_train.filter(like=\"apps\").fillna(0).columns] = X_train.filter(\n",
    "            like=\"apps\"\n",
    "        ).fillna(0)\n",
    "        X_test[X_test.filter(like=\"apps\").fillna(0).columns] = X_test.filter(\n",
    "            like=\"apps\"\n",
    "        ).fillna(0)\n",
    "        X_train.location_total_number_of_nightly_staypoints_excluding_home = X_train.location_total_number_of_nightly_staypoints_excluding_home.fillna(0)\n",
    "        X_test.location_total_number_of_nightly_staypoints_excluding_home = X_test.location_total_number_of_nightly_staypoints_excluding_home.fillna(0)\n",
    "    \n",
    "        # Perform mean imputation unless skipped\n",
    "        if not skip:\n",
    "            X_train = X_train.select_dtypes([\"int\", \"int64\", \"float\", \"float64\"])\n",
    "            X_test = X_test[X_train.columns]\n",
    "            X_train = X_train.fillna(X_train.mean().to_dict())\n",
    "            X_test = X_test.fillna(X_train.mean().to_dict())\n",
    "\n",
    "        # Save the imputed datasets back to the files\n",
    "        write_to_file(X_train, X_train_name)\n",
    "        write_to_file(X_test, X_test_name)\n",
    "\n",
    "feature_imputation(interim_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edb0f8c-47c4-4843-a142-c48ebde633c4",
   "metadata": {},
   "source": [
    "## Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98a017de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(interim_dir, fold):\n",
    "    \"\"\"\n",
    "    This function applies min-max scaling to the train/test data of a single data split.\n",
    "\n",
    "    Parameters:\n",
    "    interim_dir (str): The directory where the interim data files are stored.\n",
    "    fold (int): The fold number to read and scale the data for.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Contains the following elements:\n",
    "        - X_train (DataFrame): Scaled training data.\n",
    "        - X_test (DataFrame): Scaled test data.\n",
    "        - X_train_name (DataFrame): Metadata or names associated with the training data.\n",
    "        - X_test_name (DataFrame): Metadata or names associated with the test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read features and names for the specified fold\n",
    "    X_train, X_test, X_train_name, X_test_name = read_features(interim_dir, fold)\n",
    "\n",
    "    # Initialize the MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit the scaler on the training data and transform it\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Convert the scaled training data back to a DataFrame with the correct feature names\n",
    "    X_train = pd.DataFrame(X_train, columns=scaler.get_feature_names_out())\n",
    "\n",
    "    # Transform the test data using the same scaler and convert it to a DataFrame\n",
    "    X_test = pd.DataFrame(\n",
    "        scaler.transform(X_test), columns=scaler.get_feature_names_out()\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, X_train_name, X_test_name\n",
    "\n",
    "\n",
    "def scale_and_write_to_file(interim_dir):\n",
    "    \"\"\"\n",
    "    This function applies min-max scaling to all data splits and writes the scaled data to files.\n",
    "\n",
    "    Parameters:\n",
    "    interim_dir (str): The directory where the interim data files are stored.\n",
    "    \"\"\"\n",
    "\n",
    "    # Loop over 5 folds to scale and save the data for each fold\n",
    "    for fold in range(5):\n",
    "\n",
    "        # Apply scaling to the current fold\n",
    "        X_train, X_test, X_train_name, X_test_name = scale(interim_dir, fold)\n",
    "\n",
    "        # Write the scaled training data to a file\n",
    "        write_to_file(X_train, X_train_name)\n",
    "\n",
    "        # Write the scaled test data to a file\n",
    "        write_to_file(X_test, X_test_name)\n",
    "\n",
    "# Execute the scaling and writing process for all folds\n",
    "scale_and_write_to_file(interim_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a5f93a-3f48-49bf-92ff-71031f299932",
   "metadata": {},
   "source": [
    "## Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6846a6ea-e396-4c72-afcb-a1d7518f0022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location_total_number_of_trajectories                       5\n",
       "location_total_time_spent_outside_including_travel_hours    3\n",
       "app_number_of_apps_used                                     3\n",
       "location_mean_time_spent_stationary_hours                   3\n",
       "location_standard_deviation_time_travelled_hours            1\n",
       "location_percentage_of_staypoints_visited_once              1\n",
       "location_total_distance_travelled_kilometers                1\n",
       "app_duration_opened_all_apps_minutes_at_night               1\n",
       "app_mean_duration_opened_communication_apps_minutes         1\n",
       "location_mean_time_travelled_hours                          1\n",
       "location_total_time_spent_stationary_hours                  1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_data(input_dir, fold):\n",
    "    \"\"\"\n",
    "    This function reads and returns different parts of the data for a single fold.\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): The directory where the data files are stored.\n",
    "    fold (int): The fold number to read the data for.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Contains the following elements:\n",
    "        - X_train (DataFrame): Training data features.\n",
    "        - X_test (DataFrame): Test data features.\n",
    "        - X_train_demo (DataFrame): Training data demographic features.\n",
    "        - X_test_demo (DataFrame): Test data demographic features.\n",
    "        - y_train (ndarray): Training data labels.\n",
    "        - y_test (ndarray): Test data labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the training data for the specified fold\n",
    "    X_train = pd.read_csv(input_dir + \"X_train_\" + str(fold) + \".csv\")\n",
    "    \n",
    "    # Filter out demographic features from the training data\n",
    "    X_train_demo = X_train.filter(like=\"demo_\")\n",
    "    \n",
    "    # Read the test data for the specified fold\n",
    "    X_test = pd.read_csv(input_dir + \"X_test_\" + str(fold) + \".csv\")\n",
    "    \n",
    "    # Filter out demographic features from the test data\n",
    "    X_test_demo = X_test.filter(like=\"demo_\")\n",
    "    \n",
    "    # Read the training labels and convert them to a flat numpy array\n",
    "    y_train = pd.read_csv(input_dir + \"y_train_\" + str(fold) + \".csv\").to_numpy().ravel()\n",
    "    \n",
    "    # Read the test labels and convert them to a flat numpy array\n",
    "    y_test = pd.read_csv(input_dir + \"y_test_\" + str(fold) + \".csv\").to_numpy().ravel()\n",
    "    \n",
    "    return X_train, X_test, X_train_demo, X_test_demo, y_train, y_test\n",
    "\n",
    "def boruta_single_df(X, y, perc=100):\n",
    "    \"\"\"\n",
    "    This function applies the Boruta feature selection algorithm to a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    X (DataFrame): The input features.\n",
    "    y (ndarray): The target labels.\n",
    "    perc (int, optional): The percentage of features to consider in each iteration. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame containing only the selected features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the RandomForestClassifier model with balanced class weights\n",
    "    model = RandomForestClassifier(max_depth=5, class_weight=\"balanced\", n_jobs=-1)\n",
    "    \n",
    "    # Initialize the Boruta feature selector with the given model and parameters\n",
    "    boruta_selector = BorutaPy(\n",
    "        model, n_estimators=\"auto\", perc=perc, random_state=42, max_iter=200\n",
    "    )\n",
    "\n",
    "    # Fit the Boruta selector on the input data\n",
    "    boruta_selector.fit(X, y)\n",
    "\n",
    "    # Get the selected features' column names\n",
    "    selected_features = X.columns[boruta_selector.support_]\n",
    "\n",
    "    # Keep only the selected features in the DataFrame\n",
    "    X = X[selected_features]\n",
    "\n",
    "    return X\n",
    "\n",
    "def boruta_multiple_dfs(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    This function applies the Boruta feature selection across multiple folds and aggregates the selected features.\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): The directory where the input data files are stored.\n",
    "    output_dir (str): The directory where the output files will be saved (not used in this code).\n",
    "\n",
    "    Returns:\n",
    "    list: A list of selected features across all folds.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty list to store selected features from all folds\n",
    "    feature_list = []\n",
    "    \n",
    "    # Iterate over a range of 5 folds\n",
    "    for fold in range(5):\n",
    "\n",
    "        # Read the data for the current fold\n",
    "        X_train, X_test, X_train_demo, X_test_demo, y_train, y_test = read_data(\n",
    "            input_dir, fold\n",
    "        )\n",
    "\n",
    "        # Apply Boruta feature selection on the training data\n",
    "        X_train = boruta_single_df(X_train, y_train, perc=80)\n",
    "\n",
    "        # Extend the feature list with the selected features' names\n",
    "        feature_list.extend(X_train.columns.tolist())\n",
    "\n",
    "    return feature_list\n",
    "\n",
    "# Apply Boruta feature selection across multiple folds and get the list of selected features\n",
    "feature_list = boruta_multiple_dfs(interim_dir, processed_dir)\n",
    "\n",
    "# Count the occurrence of each feature across the folds\n",
    "pd.Series(feature_list).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9141f97c-b8a2-4920-af2e-1cda752bd60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_feature_multiple_dfs(input_dir, output_dir):\n",
    "    '''\n",
    "    This function processes training and test datasets across multiple data folds, specifically selecting and \n",
    "    saving a subset of features for each fold. The process includes the following steps:\n",
    "\n",
    "    1. Iterates through a predefined number of data folds (in this case, 5 folds).\n",
    "    2. For each fold, reads the training and test datasets along with their corresponding demographic data and labels.\n",
    "    3. Selects a subset of features from the training and test datasets, specifically combining demographic data \n",
    "       with the 'location_total_number_of_trajectories' feature.\n",
    "    4. Constructs file names for saving the processed datasets (both features and labels) for each fold.\n",
    "    5. Saves the processed training and test datasets, as well as their corresponding labels, to the specified output directory.\n",
    "\n",
    "    Parameters:\n",
    "    - input_dir (str): The directory where the input data files are stored.\n",
    "    - output_dir (str): The directory where the processed data files will be saved.\n",
    "    \n",
    "    Returns:\n",
    "    - None: The function saves the processed datasets and labels as CSV files in the specified output directory.\n",
    "    '''\n",
    "    \n",
    "    # Loop over the 5 data folds\n",
    "    for fold in range(5):\n",
    "\n",
    "        # Read the data for the current fold (training and test features, demographics, and labels)\n",
    "        X_train, X_test, X_train_demo, X_test_demo, y_train, y_test = read_data(\n",
    "            input_dir, fold\n",
    "        )\n",
    "\n",
    "        # Select and combine the demographic data and the 'location_total_number_of_trajectories' feature for the training set\n",
    "        X_train = pd.concat(\n",
    "            [X_train_demo, X_train['location_total_number_of_trajectories']], axis=1\n",
    "        )\n",
    "\n",
    "        # Select and combine the demographic data and the 'location_total_number_of_trajectories' feature for the test set\n",
    "        X_test = pd.concat(\n",
    "            [X_test_demo, X_test['location_total_number_of_trajectories']], axis=1\n",
    "        )\n",
    "\n",
    "        # Construct the file names for saving the processed training features and labels\n",
    "        X_train_name = output_dir + \"X_train_\" + str(fold) + \".csv\"\n",
    "        y_train_name = output_dir + \"y_train_\" + str(fold) + \".csv\"\n",
    "\n",
    "        # Construct the file names for saving the processed test features and labels\n",
    "        X_test_name = output_dir + \"X_test_\" + str(fold) + \".csv\"\n",
    "        y_test_name = output_dir + \"y_test_\" + str(fold) + \".csv\"\n",
    "\n",
    "        # Save the processed training features to a CSV file\n",
    "        write_to_file(X_train, X_train_name)\n",
    "\n",
    "        # Save the training labels to a CSV file\n",
    "        write_to_file(pd.DataFrame(y_train), y_train_name)\n",
    "\n",
    "        # Save the processed test features to a CSV file\n",
    "        write_to_file(X_test, X_test_name)\n",
    "\n",
    "        # Save the test labels to a CSV file\n",
    "        write_to_file(pd.DataFrame(y_test), y_test_name)\n",
    "\n",
    "# Execute the function to process and save features for all data folds\n",
    "select_feature_multiple_dfs(interim_dir, processed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa6c6d-5101-496c-b40d-4adcf2f52401",
   "metadata": {},
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adf2e22-6a63-4937-b38f-b49e12f5c228",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dba1c4ce",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n",
      "Fitting 10 folds for each of 160 candidates, totalling 1600 fits\n",
      "Fitting 10 folds for each of 160 candidates, totalling 1600 fits\n",
      "Fitting 10 folds for each of 160 candidates, totalling 1600 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n",
      "Fitting 10 folds for each of 160 candidates, totalling 1600 fits\n",
      "Fitting 10 folds for each of 160 candidates, totalling 1600 fits\n",
      "Fitting 10 folds for each of 160 candidates, totalling 1600 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n",
      "Fitting 10 folds for each of 160 candidates, totalling 1600 fits\n",
      "Fitting 10 folds for each of 160 candidates, totalling 1600 fits\n",
      "Fitting 10 folds for each of 160 candidates, totalling 1600 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n",
      "Fitting 10 folds for each of 160 candidates, totalling 1600 fits\n",
      "Fitting 10 folds for each of 160 candidates, totalling 1600 fits\n",
      "Fitting 10 folds for each of 160 candidates, totalling 1600 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n",
      "Fitting 10 folds for each of 160 candidates, totalling 1600 fits\n",
      "Fitting 10 folds for each of 160 candidates, totalling 1600 fits\n",
      "Fitting 10 folds for each of 160 candidates, totalling 1600 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def build_model(model_type):\n",
    "    '''\n",
    "    This function builds and configures a machine learning model based on the specified model type.\n",
    "    It performs the following steps:\n",
    "\n",
    "    1. Sets up a Stratified K-Fold cross-validation scheme to ensure balanced class representation across splits.\n",
    "    2. Defines a set of hyperparameters to be tuned during model training, specific to the type of model.\n",
    "    3. Initializes the model with appropriate settings (e.g., logistic regression, random forest, or dummy classifier).\n",
    "    4. Wraps the model in a `GridSearchCV` object to perform an exhaustive search over the specified hyperparameters.\n",
    "    5. Creates a pipeline that includes Min-Max scaling followed by the model with hyperparameter tuning.\n",
    "\n",
    "    Parameters:\n",
    "    - model_type (str): The type of model to build, which can be \"lr\" (logistic regression), \"rf\" (random forest), or \"dummy\" (baseline model).\n",
    "\n",
    "    Returns:\n",
    "    - Pipeline: A scikit-learn pipeline object that includes data scaling and the model wrapped in a grid search for hyperparameter tuning.\n",
    "    '''\n",
    "    \n",
    "    # Define a stratified K-Fold cross-validation strategy with 10 splits\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    if model_type == \"lr\":\n",
    "        # Define hyperparameters for Logistic Regression\n",
    "        parameters = {\n",
    "            \"penalty\": [\"elasticnet\"],\n",
    "            \"C\": np.logspace(-3, 3, num=6),\n",
    "            \"l1_ratio\": np.arange(0.01, 1, 0.1),\n",
    "            \"solver\": [\"saga\"],\n",
    "        }\n",
    "\n",
    "        # Initialize Logistic Regression model with balanced class weights\n",
    "        model = LogisticRegression(\n",
    "            max_iter=10000, class_weight=\"balanced\", random_state=42\n",
    "        )\n",
    "  \n",
    "    elif model_type == \"rf\":\n",
    "        # Define hyperparameters for Random Forest\n",
    "        parameters = {\n",
    "            \"max_depth\": [5, 10, 15, 20],\n",
    "            \"max_features\": [\"sqrt\", \"log2\"],\n",
    "            \"min_samples_leaf\": [5, 10, 15, 20],\n",
    "            \"n_estimators\": np.arange(100, 600, 100),\n",
    "        }\n",
    "\n",
    "        # Initialize Random Forest model with balanced class weights\n",
    "        model = RandomForestClassifier(random_state=42, class_weight=\"balanced\")\n",
    "     \n",
    "    elif model_type == 'dummy':\n",
    "        # Define parameters for Dummy Classifier (baseline model)\n",
    "        parameters = {'strategy': [\"uniform\"]}\n",
    "        \n",
    "        # Initialize Dummy Classifier\n",
    "        model = DummyClassifier(random_state=42)\n",
    " \n",
    "    # Wrap the model in GridSearchCV to perform hyperparameter tuning\n",
    "    model = GridSearchCV(\n",
    "        model, parameters, scoring='roc_auc', verbose=4, n_jobs=-1, cv=cv\n",
    "    )\n",
    "    \n",
    "    # Create a pipeline that scales the data and then applies the model\n",
    "    model = make_pipeline(MinMaxScaler(), model)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_model(model, X, y):\n",
    "    '''\n",
    "    This function fits the provided model to the training data.\n",
    "\n",
    "    Parameters:\n",
    "    - model (Pipeline): The scikit-learn pipeline object that includes data scaling and model training.\n",
    "    - X (DataFrame): The feature matrix for training.\n",
    "    - y (Series or ndarray): The target labels for training.\n",
    "\n",
    "    Returns:\n",
    "    - Pipeline: The trained model.\n",
    "    '''\n",
    "    return model.fit(X, y)\n",
    "\n",
    "\n",
    "def save_model(model, model_dir, model_type, modality, fold):\n",
    "    '''\n",
    "    This function saves the trained model to a specified directory in a serialized format (pickle).\n",
    "\n",
    "    Parameters:\n",
    "    - model (Pipeline): The trained scikit-learn pipeline object.\n",
    "    - model_dir (str): The directory where the model will be saved.\n",
    "    - model_type (str): The type of model (e.g., \"lr\", \"rf\", \"dummy\").\n",
    "    - modality (str): The modality or feature set used for training (e.g., \"demo\", \"dpd\", \"all\").\n",
    "    - fold (int): The fold number corresponding to the cross-validation split.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function saves the model as a .pkl file in the specified directory.\n",
    "    '''\n",
    "    \n",
    "    # Create the filename for the model based on model type, modality, and fold\n",
    "    file_path = model_dir + model_type + \"_\" + modality + \"_\" + str(fold) + \".pkl\"\n",
    "    \n",
    "    # Save the model as a pickle file\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "\n",
    "def train_models(processed_dir, model_dir, model_types, modalities):\n",
    "    '''\n",
    "    This function orchestrates the training of multiple models across different modalities and cross-validation folds.\n",
    "    It performs the following steps:\n",
    "\n",
    "    1. Iterates over each fold in the cross-validation scheme.\n",
    "    2. Reads the processed training data and labels for the current fold.\n",
    "    3. For each model type (e.g., logistic regression, random forest, dummy classifier), and for each modality \n",
    "       (e.g., demographic data, digital phenotyping data, or all data), the function builds, fits, and saves the model.\n",
    "    4. Depending on the modality, it selects appropriate features for training.\n",
    "\n",
    "    Parameters:\n",
    "    - processed_dir (str): The directory where the processed data files are stored.\n",
    "    - model_dir (str): The directory where the trained models will be saved.\n",
    "    - model_types (list): A list of model types to be trained (e.g., [\"dummy\", \"lr\", \"rf\"]).\n",
    "    - modalities (list): A list of modalities to be used for training (e.g., [\"demo\", \"dpd\", \"all\"]).\n",
    "    \n",
    "    Returns:\n",
    "    - None: The function trains and saves models for all specified combinations of model types and modalities.\n",
    "    '''\n",
    "    \n",
    "    # Loop over the 5 cross-validation folds\n",
    "    for fold in range(5):\n",
    "\n",
    "        # Read the training features and labels for the current fold\n",
    "        X = pd.read_csv(processed_dir + \"X_train_\" + str(fold) + \".csv\")\n",
    "        y = pd.read_csv(processed_dir + \"y_train_\" + str(fold) + \".csv\").squeeze()\n",
    "\n",
    "        # Loop over each model type to be trained\n",
    "        for model_type in model_types:\n",
    "\n",
    "            # Loop over each modality to be used for training\n",
    "            for modality in modalities:\n",
    "\n",
    "                # Build the model based on the specified type\n",
    "                model = build_model(model_type)\n",
    "\n",
    "                if modality == \"dpd\":\n",
    "                    # Exclude demographic features for the \"dpd\" modality\n",
    "                    X_ = X[X.columns[~X.columns.isin(X.filter(like=\"demo\").columns)]]\n",
    "                    # Fit the model using the selected features\n",
    "                    model = fit_model(model, X_, y)\n",
    "\n",
    "                elif modality == \"demo\":\n",
    "                    # Fit the model using only demographic features\n",
    "                    model = fit_model(model, X.filter(like=modality), y)\n",
    "\n",
    "                else:\n",
    "                    # Fit the model using all features\n",
    "                    model = fit_model(model, X, y)\n",
    "\n",
    "                # Save the trained model to the specified directory\n",
    "                save_model(model, model_dir, model_type, modality, fold)\n",
    "\n",
    "\n",
    "# General set-up for training models\n",
    "model_types = [\"dummy\", \"lr\", \"rf\"]  # Model types: Dummy, Logistic Regression, Random Forest\n",
    "modalities = [\"demo\", \"dpd\", \"all\"]  # Modalities: Demographic data, Digital phenotyping data, All data\n",
    "\n",
    "# Train and save models for all specified combinations of model types and modalities\n",
    "train_models(processed_dir, model_dir, model_types, modalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd164938-7924-4245-aa1a-253ac2302c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    '''\n",
    "    This function loads a serialized Python object from a pickle file.\n",
    "    \n",
    "    Parameters:\n",
    "    - file (str): The path to the pickle file that contains the serialized object.\n",
    "    \n",
    "    Returns:\n",
    "    - object: The deserialized Python object from the pickle file.\n",
    "    '''\n",
    "    \n",
    "    # Open the file in binary read mode and load the object using pickle\n",
    "    with open(file, \"rb\") as f:\n",
    "        object = pickle.load(f)\n",
    "\n",
    "    return object\n",
    "\n",
    "def get_selected_hyperparameters(model_dir):\n",
    "    '''\n",
    "    This function extracts the best hyperparameters from models saved in the specified directory and compiles them into a CSV file.\n",
    "    It performs the following steps:\n",
    "\n",
    "    1. Initializes an empty DataFrame to store the hyperparameters for all models.\n",
    "    2. Lists all files in the specified model directory, ignoring the first entry (assumed to be a non-model file).\n",
    "    3. Iterates through each model file, unpickling the model to access its best hyperparameters.\n",
    "    4. Extracts the model type and feature set from the filename and combines this information with the hyperparameters.\n",
    "    5. Appends the information for each model to a growing DataFrame.\n",
    "    6. After processing all models, saves the compiled hyperparameters to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - model_dir (str): The directory where the model pickle files are stored.\n",
    "    \n",
    "    Returns:\n",
    "    - None: The function saves a CSV file named \"model-hyperparameters.csv\" containing the hyperparameters for all models.\n",
    "    '''\n",
    "    \n",
    "    # Initialize an empty DataFrame to store hyperparameters for all models\n",
    "    big_df = pd.DataFrame()\n",
    "    \n",
    "    # List all files in the model directory, ignoring the first entry\n",
    "    model_list = os.listdir(model_dir)[1:]\n",
    "    \n",
    "    # Iterate through each model file in the directory\n",
    "    for model_path in model_list:\n",
    "        # Load the model from the pickle file\n",
    "        model = unpickle(model_dir + model_path)\n",
    "        \n",
    "        # Extract feature type and model type from the file name\n",
    "        df1 = pd.DataFrame(\n",
    "            {\"features\": model_path.split(\"_\")[1], \"model\": model_path.split(\"_\")[0]},\n",
    "            index=[0],\n",
    "        )\n",
    "        \n",
    "        # Extract the best hyperparameters from the model\n",
    "        df2 = pd.DataFrame(model[1].best_params_, index=[0])\n",
    "        \n",
    "        # Combine the model info and hyperparameters into a single DataFrame\n",
    "        df = pd.concat([df1, df2], axis=1)\n",
    "        \n",
    "        # Append this information to the main DataFrame\n",
    "        big_df = pd.concat([big_df, df], axis=0)\n",
    "        \n",
    "        # Reset the index of the DataFrame for proper alignment\n",
    "        big_df = big_df.reset_index(drop=True)\n",
    "    \n",
    "    # Save the compiled hyperparameters to a CSV file\n",
    "    big_df.to_csv(\"model-hyperparameters.csv\", index=False)\n",
    "\n",
    "# Call the function to extract hyperparameters and save them to a CSV file\n",
    "get_selected_hyperparameters(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b389fdbd-da82-45f2-8832-e06d1b0179bf",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "Here, we apply the trained models to our validation and test data. The validation data is the data that is used to tune hyperparameters with 10-fold stratified cross-validation (CV). The test data is the data that has not been used for tuning the model hyperparameters. When models perform much better on validation than test data, this means the model is overfitted on the validation data. We here see that model performance on validation and test data is similar and conclude the models have not been overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0160a539-a835-4543-ba8a-6839bb0beb0a",
   "metadata": {},
   "source": [
    "## Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9744f213-4b0c-4cac-ba82-10e9dfb8e7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x900 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_single_row_results(model_dir, row):\n",
    "    '''\n",
    "    This function retrieves and summarizes the performance of a specific model from a directory.\n",
    "    It performs the following steps:\n",
    "\n",
    "    1. Lists all files in the specified model directory and filters out only the pickle files.\n",
    "    2. Resets the index of the filtered paths for consistent referencing.\n",
    "    3. Loads the model from the specified row using the `unpickle()` function.\n",
    "    4. Extracts the model's file name and its best performance metric (e.g., AUC) from the GridSearchCV results.\n",
    "    5. Returns a DataFrame containing the model's name and its corresponding performance metric.\n",
    "\n",
    "    Parameters:\n",
    "    - model_dir (str): The directory where the model pickle files are stored.\n",
    "    - row (int): The index of the model in the directory list to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame with a single row containing the model's name and its best performance metric.\n",
    "    '''\n",
    "    \n",
    "    # List all files in the model directory and filter for pickle files\n",
    "    paths = model_dir + pd.Series(os.listdir(model_dir))\n",
    "    paths = paths[paths.str.contains(\".pkl\")]\n",
    "    paths = paths.reset_index(drop=True)\n",
    "\n",
    "    # Load the model from the specified row\n",
    "    study = unpickle(paths[row])\n",
    "\n",
    "    # Return a DataFrame with the model name and its best score (e.g., AUC)\n",
    "    return pd.DataFrame(\n",
    "        {\"model\": paths[row].split(\"/\")[-1], \"metric\": study[1].best_score_},\n",
    "        index=[row],\n",
    "    )\n",
    "\n",
    "\n",
    "def make_multiple_row_results(model_dir):\n",
    "    '''\n",
    "    This function compiles the performance metrics of all models in a directory into a single DataFrame.\n",
    "    It performs the following steps:\n",
    "\n",
    "    1. Initializes an empty DataFrame to store the results.\n",
    "    2. Iterates through all models in the specified directory (except the first one).\n",
    "    3. For each model, calls `make_single_row_results()` to retrieve the model's name and best performance metric.\n",
    "    4. Concatenates the results for each model into the main DataFrame.\n",
    "    5. Sorts the resulting DataFrame by performance metric in descending order and resets the index.\n",
    "\n",
    "    Parameters:\n",
    "    - model_dir (str): The directory where the model pickle files are stored.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing the names and performance metrics of all models, sorted by metric.\n",
    "    '''\n",
    "    \n",
    "    # Initialize an empty DataFrame to store results\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Get the number of model files in the directory\n",
    "    n = len(os.listdir(model_dir))\n",
    "\n",
    "    # Iterate over each model, except the first file\n",
    "    for i in range(n - 1):\n",
    "        # Retrieve and concatenate the results for each model\n",
    "        df = pd.concat([df, make_single_row_results(model_dir, i)], axis=0)\n",
    "\n",
    "    # Sort the DataFrame by performance metric and reset the index\n",
    "    return df.sort_values(by=\"metric\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def evaluate_all_models_on_validation_data(model_dir):\n",
    "    '''\n",
    "    This function evaluates the performance of all models on validation data and organizes the results for comparison.\n",
    "    It performs the following steps:\n",
    "\n",
    "    1. Calls `make_multiple_row_results()` to compile the performance metrics of all models in the directory.\n",
    "    2. Initializes an empty DataFrame to store the evaluation results.\n",
    "    3. Iterates through each model type (e.g., dummy, random forest, logistic regression) and modality (e.g., all, demo, dpd).\n",
    "    4. Filters the compiled metrics DataFrame for each combination of model type, modality, and fold.\n",
    "    5. Appends the performance results (AUC) for each combination to the evaluation DataFrame.\n",
    "    6. Returns a DataFrame containing the performance metrics for all models, organized by modality and model type.\n",
    "\n",
    "    Parameters:\n",
    "    - model_dir (str): The directory where the model pickle files are stored.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing the performance metrics (AUC) for all models, organized by modality and model type.\n",
    "    '''\n",
    "    \n",
    "    # Compile performance metrics for all models\n",
    "    df = make_multiple_row_results(model_dir)\n",
    "\n",
    "    # Define the models and modalities to evaluate\n",
    "    models = [\"dummy\", \"rf\", \"lr\"]\n",
    "    modalities = [\"all\", \"demo\", \"dpd\"]\n",
    "\n",
    "    # Initialize an empty DataFrame to store evaluation results\n",
    "    performance_validation_sets = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each model type\n",
    "    for model in models:\n",
    "        # Iterate over each modality\n",
    "        for modality in modalities:\n",
    "            # Iterate over each fold (assumes 5-fold cross-validation)\n",
    "            for fold in range(5):\n",
    "                # Filter the metrics DataFrame for the current combination of model, modality, and fold\n",
    "                performance_multiple_folds = df[\n",
    "                    df.model.str.contains(model)\n",
    "                    & df.model.str.contains(modality)\n",
    "                    & df.model.str.contains(str(fold))\n",
    "                ]\n",
    "\n",
    "                # Append the performance results to the evaluation DataFrame\n",
    "                performance_validation_sets = pd.concat(\n",
    "                    [\n",
    "                        performance_validation_sets,\n",
    "                        pd.DataFrame(\n",
    "                            {\n",
    "                                \"modality\": modality[:3].upper(),\n",
    "                                \"model\": model.upper(),\n",
    "                                \"auc\": performance_multiple_folds.metric.values[0],\n",
    "                            },\n",
    "                            index=[0],\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "    return performance_validation_sets\n",
    "\n",
    "\n",
    "# Evaluate all models on validation data and organize the results\n",
    "df = evaluate_all_models_on_validation_data(model_dir)\n",
    "\n",
    "# Sort the results by modality and model type\n",
    "df.sort_values(by=[\"modality\", \"model\"]).reset_index(drop=True)\n",
    "\n",
    "# Create a FacetGrid for visualizing the AUC scores by modality and model type\n",
    "g = sns.FacetGrid(data=df, row=\"modality\", row_order=[\"ALL\", \"DPD\", \"DEM\"])\n",
    "g.map(sns.boxplot, \"auc\", \"model\", orient=\"h\", order=[\"RF\", \"LR\"])\n",
    "\n",
    "# Set the x-axis limits and display the plot\n",
    "plt.xlim(0.5, 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc5261b-fae6-4716-ae1d-95827b43a01a",
   "metadata": {},
   "source": [
    "## Test set\n",
    "\n",
    "We evaluate models on hold-out test data. We have 5 train-test splits so we evaluate 5 times and aggregate by taking the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cab93b6-2b5a-41ab-a5b1-8bb64d8f9e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    '''\n",
    "    This function loads a serialized Python object from a pickle file.\n",
    "    \n",
    "    Parameters:\n",
    "    - file (str): The path to the pickle file that contains the serialized object.\n",
    "    \n",
    "    Returns:\n",
    "    - object: The deserialized Python object from the pickle file.\n",
    "    '''\n",
    "    \n",
    "    # Open the file in binary read mode and load the object using pickle\n",
    "    with open(file, \"rb\") as f:\n",
    "        object = pickle.load(f)\n",
    "\n",
    "    return object\n",
    "\n",
    "\n",
    "def load_model(path):\n",
    "    '''\n",
    "    This function loads a machine learning model from a specified pickle file.\n",
    "\n",
    "    Parameters:\n",
    "    - path (str): The path to the pickle file containing the model.\n",
    "\n",
    "    Returns:\n",
    "    - model: The loaded machine learning model.\n",
    "    '''\n",
    "    return unpickle(path)\n",
    "\n",
    "\n",
    "def predict_model(processed_data_dir, model_dir, modality, model_type, fold):\n",
    "    '''\n",
    "    This function loads a trained model and makes predictions on the test data.\n",
    "    It performs the following steps:\n",
    "\n",
    "    1. Loads the test data (features and labels) for the specified fold.\n",
    "    2. Loads the trained model corresponding to the specified modality, model type, and fold.\n",
    "    3. Depending on the modality, selects the relevant features from the test data.\n",
    "    4. Uses the loaded model to make predictions and compute probabilities on the test data.\n",
    "    5. Returns the true labels, predicted labels, and predicted probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - processed_data_dir (str): The directory where the processed test data files are stored.\n",
    "    - model_dir (str): The directory where the trained model pickle files are stored.\n",
    "    - modality (str): The modality or feature set used for prediction (e.g., \"demo\", \"dpd\", \"all\").\n",
    "    - model_type (str): The type of model used (e.g., \"lr\", \"rf\", \"dummy\").\n",
    "    - fold (int): The fold number corresponding to the cross-validation split.\n",
    "\n",
    "    Returns:\n",
    "    - y_test (DataFrame): The true labels for the test data.\n",
    "    - y_pred (ndarray): The predicted labels from the model.\n",
    "    - y_score (ndarray): The predicted probabilities from the model.\n",
    "    '''\n",
    "    \n",
    "    # Load the test features and labels for the specified fold\n",
    "    X_test = pd.read_csv(processed_data_dir + \"X_test_\" + str(fold) + \".csv\")\n",
    "    y_test = pd.read_csv(processed_data_dir + \"y_test_\" + str(fold) + \".csv\")\n",
    "\n",
    "    # Load the trained model\n",
    "    model = load_model(\n",
    "        model_dir + model_type + \"_\" + modality + \"_\" + str(fold) + \".pkl\"\n",
    "    )\n",
    "\n",
    "    # Select and predict based on the specified modality\n",
    "    if modality == \"dpd\":\n",
    "        # Exclude demographic features for the \"dpd\" modality\n",
    "        X_test = X_test[\n",
    "            X_test.columns[~X_test.columns.isin(X_test.filter(like=\"demo\").columns)]\n",
    "        ]\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_score = model.predict_proba(X_test)\n",
    "\n",
    "    elif modality == \"all\":\n",
    "        # Use all features for prediction\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_score = model.predict_proba(X_test)\n",
    "\n",
    "    elif modality == \"demo\":\n",
    "        # Use only demographic features for prediction\n",
    "        y_pred = model.predict(X_test.filter(like=modality))\n",
    "        y_score = model.predict_proba(X_test.filter(like=modality))\n",
    "\n",
    "    return y_test, y_pred, y_score\n",
    "\n",
    "\n",
    "def compute_performance(y_test, y_pred, y_score, model_type, modality, fold):\n",
    "    '''\n",
    "    This function computes various performance metrics for the model's predictions.\n",
    "    It calculates metrics such as AUC-ROC, F1 scores, precision, recall, and accuracy.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_test (DataFrame): The true labels for the test data.\n",
    "    - y_pred (ndarray): The predicted labels from the model.\n",
    "    - y_score (ndarray): The predicted probabilities from the model.\n",
    "    - model_type (str): The type of model used (e.g., \"lr\", \"rf\", \"dummy\").\n",
    "    - modality (str): The modality or feature set used for prediction (e.g., \"demo\", \"dpd\", \"all\").\n",
    "    - fold (int): The fold number corresponding to the cross-validation split.\n",
    "\n",
    "    Returns:\n",
    "    - df (DataFrame): A DataFrame containing the computed performance metrics.\n",
    "    '''\n",
    "    \n",
    "    # Calculate various performance metrics\n",
    "    auc_roc = roc_auc_score(y_test, y_score[:, 1])\n",
    "    f11 = f1_score(y_test, y_pred, pos_label=1)\n",
    "    f10 = f1_score(y_test, y_pred, pos_label=0)\n",
    "    precision1 = precision_score(y_test, y_pred, pos_label=1)\n",
    "    precision0 = precision_score(y_test, y_pred, pos_label=0)\n",
    "    recall1 = recall_score(y_test, y_pred, pos_label=1)\n",
    "    recall0 = recall_score(y_test, y_pred, pos_label=0)\n",
    "    accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Create a DataFrame with the performance metrics\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Modality\": modality,\n",
    "            \"Fold\": fold,\n",
    "            \"Model\": model_type,\n",
    "            \"AUC ROC\": auc_roc,\n",
    "            \"F10\": f10,\n",
    "            \"F11\": f11,\n",
    "            \"Precision0\": precision0,\n",
    "            \"Precision1\": precision1,\n",
    "            \"Recall0\": recall0,\n",
    "            \"Recall1\": recall1,\n",
    "            \"Accuracy\": accuracy\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def write_predictions_to_file(\n",
    "    y_test, y_pred, y_score, evaluation_dir, modality, model_type, fold\n",
    "):\n",
    "    '''\n",
    "    This function saves the true labels, predicted labels, and predicted probabilities to CSV files.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_test (DataFrame): The true labels for the test data.\n",
    "    - y_pred (ndarray): The predicted labels from the model.\n",
    "    - y_score (ndarray): The predicted probabilities from the model.\n",
    "    - evaluation_dir (str): The directory where the prediction results will be saved.\n",
    "    - modality (str): The modality or feature set used for prediction (e.g., \"demo\", \"dpd\", \"all\").\n",
    "    - model_type (str): The type of model used (e.g., \"lr\", \"rf\", \"dummy\").\n",
    "    - fold (int): The fold number corresponding to the cross-validation split.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function saves the predictions to CSV files.\n",
    "    '''\n",
    "    \n",
    "    # Convert the true labels, predicted labels, and predicted scores to DataFrames\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    y_pred = pd.DataFrame(y_pred)\n",
    "    y_score = pd.DataFrame(y_score)\n",
    "\n",
    "    # Save the true labels to a CSV file\n",
    "    y_test.to_csv(\n",
    "        evaluation_dir\n",
    "        + \"y_test_\"\n",
    "        + modality\n",
    "        + \"_\"\n",
    "        + model_type\n",
    "        + \"_\"\n",
    "        + str(fold)\n",
    "        + \".csv\",\n",
    "        index=False,\n",
    "    )\n",
    "    # Save the predicted labels to a CSV file\n",
    "    y_pred.to_csv(\n",
    "        evaluation_dir\n",
    "        + \"y_pred_\"\n",
    "        + modality\n",
    "        + \"_\"\n",
    "        + model_type\n",
    "        + \"_\"\n",
    "        + str(fold)\n",
    "        + \".csv\",\n",
    "        index=False,\n",
    "    )\n",
    "    # Save the predicted probabilities to a CSV file\n",
    "    y_score.to_csv(\n",
    "        evaluation_dir\n",
    "        + \"y_pred_\"\n",
    "        + modality\n",
    "        + \"_\"\n",
    "        + model_type\n",
    "        + \"_\"\n",
    "        + str(fold)\n",
    "        + \".csv\",\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_performance_single_model(model_dir, model_type, modality, data_dir, fold):\n",
    "    '''\n",
    "    This function evaluates a single model on test data, computes its performance metrics, and saves the results.\n",
    "    It performs the following steps:\n",
    "\n",
    "    1. Calls `predict_model()` to make predictions on the test data.\n",
    "    2. Saves the true labels, predicted labels, and predicted probabilities to files.\n",
    "    3. Computes various performance metrics using `compute_performance()`.\n",
    "    4. Returns the computed performance metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - model_dir (str): The directory where the trained model pickle files are stored.\n",
    "    - model_type (str): The type of model used (e.g., \"lr\", \"rf\", \"dummy\").\n",
    "    - modality (str): The modality or feature set used for prediction (e.g., \"demo\", \"dpd\", \"all\").\n",
    "    - data_dir (str): The directory where the processed test data files are stored.\n",
    "    - fold (int): The fold number corresponding to the cross-validation split.\n",
    "\n",
    "    Returns:\n",
    "    - performance (DataFrame): A DataFrame containing the computed performance metrics for the model.\n",
    "    '''\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_test, y_pred, y_score = predict_model(\n",
    "        processed_dir, model_dir, modality, model_type, fold\n",
    "    )\n",
    "\n",
    "    # Save the predictions to files\n",
    "    write_predictions_to_file(\n",
    "        y_test, y_pred, y_score, evaluation_dir, modality, model_type, fold\n",
    "    )\n",
    "\n",
    "    # Compute the performance metrics\n",
    "    performance = compute_performance(\n",
    "        y_test, y_pred, y_score, model_type, modality, fold\n",
    "    )\n",
    "        \n",
    "    return performance\n",
    "\n",
    "\n",
    "def get_performance_multiple_models(data_dir, model_dir):\n",
    "    '''\n",
    "    This function evaluates multiple models across different folds, modalities, and model types.\n",
    "    It performs the following steps:\n",
    "\n",
    "    1. Initializes an empty DataFrame to store the performance metrics.\n",
    "    2. Iterates over each fold in the cross-validation process.\n",
    "    3. For each fold, iterates through the different model types (e.g., logistic regression, random forest, dummy classifier).\n",
    "    4. For each model type, iterates through the different modalities (e.g., all features, demographic features, digital phenotyping features).\n",
    "    5. Calls `get_performance_single_model()` to evaluate each model on the test data and retrieve performance metrics.\n",
    "    6. Concatenates the performance metrics of all models into a single DataFrame.\n",
    "    7. Sorts the DataFrame by modality and model type, and resets the index for clarity.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dir (str): The directory where the processed test data files are stored.\n",
    "    - model_dir (str): The directory where the trained model pickle files are stored.\n",
    "\n",
    "    Returns:\n",
    "    - df (DataFrame): A DataFrame containing the performance metrics for all evaluated models, sorted by modality and model type.\n",
    "    '''\n",
    "    \n",
    "    # Initialize an empty DataFrame to collect performance metrics from all models\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over the cross-validation folds (assumes 5-fold CV)\n",
    "    for fold in range(5):\n",
    "        # Iterate over each model type\n",
    "        for model_type in [\"dummy\", \"lr\", \"rf\"]:\n",
    "            # Iterate over each modality\n",
    "            for modality in [\"all\", \"demo\", \"dpd\"]:\n",
    "                # Evaluate the model and get performance metrics\n",
    "                performance = get_performance_single_model(\n",
    "                    model_dir, model_type, modality, data_dir, fold\n",
    "                )\n",
    "                # Append the performance metrics to the DataFrame\n",
    "                df = pd.concat([df, performance], axis=0)\n",
    "    \n",
    "    # Sort the DataFrame by modality and model type, then reset the index\n",
    "    return df.sort_values(by=[\"Modality\", \"Model\"], ascending=True).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "\n",
    "# Evaluate all models, group by modality and model type, and compute median performance\n",
    "df = get_performance_multiple_models(processed_dir, model_dir)\n",
    "\n",
    "# Group the results by modality and model type, compute median values for performance metrics, and round them to 2 decimal places\n",
    "df = (\n",
    "    df.groupby([\"Modality\", \"Model\"])\n",
    "    .aggregate([\"median\"])\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "# Reset the index, drop the \"Fold\" column, and remove the level indicator in the column names\n",
    "df = df.reset_index().drop([\"Fold\"], axis=1).droplevel(level=1, axis=1)\n",
    "\n",
    "# Reorder and select specific rows for the final output, and save to a CSV file\n",
    "df.iloc[[0, 1, 2, 7, 8, 4, 5], :].to_csv(performance_dir + \"model-performance.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0200220-8f31-48c7-aff0-dde2e5285e9f",
   "metadata": {},
   "source": [
    "## Model fairness\n",
    "\n",
    "We assess model fairness by computing the evaluation metrics for (median-split) demographic groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a070ba4c-50a9-41f9-a9db-9b10a53d149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model_subgroup(processed_data_dir, model_dir, modality, model_type, fold, group_var, group_number):\n",
    "    '''\n",
    "    This function predicts outcomes for a specific subgroup within the test data using a pre-trained model.\n",
    "    It performs the following steps:\n",
    "\n",
    "    1. Loads the test data (features and labels) for the specified fold.\n",
    "    2. Prepares the demographic data and assigns group labels based on age, education, and sex.\n",
    "    3. Merges the test data with the demographic information to identify the indices of the selected subgroup.\n",
    "    4. Filters the test data to include only the selected subgroup.\n",
    "    5. Loads the trained model corresponding to the specified modality, model type, and fold.\n",
    "    6. Depending on the modality, selects the relevant features from the test data.\n",
    "    7. Uses the loaded model to make predictions and compute probabilities for the subgroup.\n",
    "    8. Returns the true labels, predicted labels, and predicted probabilities for the subgroup.\n",
    "\n",
    "    Parameters:\n",
    "    - processed_data_dir (str): The directory where the processed test data files are stored.\n",
    "    - model_dir (str): The directory where the trained model pickle files are stored.\n",
    "    - modality (str): The modality or feature set used for prediction (e.g., \"demo\", \"dpd\", \"all\").\n",
    "    - model_type (str): The type of model used (e.g., \"lr\", \"rf\", \"dummy\").\n",
    "    - fold (int): The fold number corresponding to the cross-validation split.\n",
    "    - group_var (str): The variable used to define the subgroup (e.g., \"age_group\", \"edu_group\", \"sex_group\").\n",
    "    - group_number (int): The specific group number within the subgroup variable (e.g., 0 or 1).\n",
    "\n",
    "    Returns:\n",
    "    - y_test (DataFrame): The true labels for the test data in the selected subgroup.\n",
    "    - y_pred (ndarray): The predicted labels from the model for the selected subgroup.\n",
    "    - y_score (ndarray): The predicted probabilities from the model for the selected subgroup.\n",
    "    '''\n",
    "    \n",
    "    # Load the test features and labels for the specified fold\n",
    "    X_test = pd.read_csv(processed_data_dir + \"X_test_\" + str(fold) + \".csv\")\n",
    "    y_test = pd.read_csv(processed_data_dir + \"y_test_\" + str(fold) + \".csv\")\n",
    "\n",
    "    # Prepare the demographic data and assign group labels\n",
    "    df = make_interim_data(raw_dir, raw_data_paths)\n",
    "    df['age_group'] = pd.cut(df.demo_age, 2, labels=[0, 1])\n",
    "    df['edu_group'] = pd.cut(df.demo_edu, 2, labels=[0, 1])\n",
    "    df['sex_group'] = df.demo_sex\n",
    "\n",
    "    # Load the IDs for the current fold and merge with demographic data to identify subgroup\n",
    "    df_id = pd.read_csv(processed_dir + 'id_test_' + str(fold) + '.csv')\n",
    "    df = pd.merge(df_id, df, on='id')[['id', 'age_group', 'edu_group', 'sex_group']]\n",
    "\n",
    "    # Select the indices corresponding to the specified subgroup\n",
    "    selected_indices = df[df[group_var] == group_number].index\n",
    "\n",
    "    # Filter the test data for the selected subgroup\n",
    "    X_test = X_test.iloc[selected_indices, :]\n",
    "    y_test = y_test.iloc[selected_indices, :]\n",
    "\n",
    "    # Load the trained model for the specified fold, modality, and model type\n",
    "    model = load_model(\n",
    "        model_dir + model_type + \"_\" + modality + \"_\" + str(fold) + \".pkl\"\n",
    "    )\n",
    "\n",
    "    # Select and predict based on the specified modality\n",
    "    if modality == \"dpd\":\n",
    "        X_test = X_test[\n",
    "            X_test.columns[~X_test.columns.isin(X_test.filter(like=\"demo\").columns)]\n",
    "        ]\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_score = model.predict_proba(X_test)\n",
    "\n",
    "    elif modality == \"all\":\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_score = model.predict_proba(X_test)\n",
    "\n",
    "    elif modality == \"demo\":\n",
    "        y_pred = model.predict(X_test.filter(like=modality))\n",
    "        y_score = model.predict_proba(X_test.filter(like=modality))\n",
    "\n",
    "    return y_test, y_pred, y_score\n",
    "\n",
    "\n",
    "def get_performance_single_model_subgroup(model_dir, model_type, modality, data_dir, fold, group_var, group_number):\n",
    "    '''\n",
    "    This function evaluates the performance of a single model on a specific subgroup within the test data.\n",
    "    It performs the following steps:\n",
    "\n",
    "    1. Calls `predict_model_subgroup()` to make predictions on the test data for the specified subgroup.\n",
    "    2. Computes various performance metrics using `compute_performance()`.\n",
    "    3. Returns the computed performance metrics for the model on the selected subgroup.\n",
    "\n",
    "    Parameters:\n",
    "    - model_dir (str): The directory where the trained model pickle files are stored.\n",
    "    - model_type (str): The type of model used (e.g., \"lr\", \"rf\", \"dummy\").\n",
    "    - modality (str): The modality or feature set used for prediction (e.g., \"demo\", \"dpd\", \"all\").\n",
    "    - data_dir (str): The directory where the processed test data files are stored.\n",
    "    - fold (int): The fold number corresponding to the cross-validation split.\n",
    "    - group_var (str): The variable used to define the subgroup (e.g., \"age_group\", \"edu_group\", \"sex_group\").\n",
    "    - group_number (int): The specific group number within the subgroup variable (e.g., 0 or 1).\n",
    "\n",
    "    Returns:\n",
    "    - performance (DataFrame): A DataFrame containing the computed performance metrics for the model on the selected subgroup.\n",
    "    '''\n",
    "    \n",
    "    # Predict on the test data for the specified subgroup\n",
    "    y_test, y_pred, y_score = predict_model_subgroup(\n",
    "        processed_dir, model_dir, modality, model_type, fold, group_var, group_number\n",
    "    )\n",
    "\n",
    "    # Compute the performance metrics for the selected subgroup\n",
    "    performance = compute_performance(\n",
    "        y_test, y_pred, y_score, model_type, modality, fold\n",
    "    )\n",
    "        \n",
    "    return performance\n",
    "\n",
    "\n",
    "def get_performance_multiple_models_subgroup(data_dir, model_dir, group_var, group_number):\n",
    "    '''\n",
    "    This function evaluates the performance of multiple models across different folds and modalities for a specific subgroup.\n",
    "    It performs the following steps:\n",
    "\n",
    "    1. Initializes an empty DataFrame to store the performance metrics.\n",
    "    2. Iterates over each fold in the cross-validation process.\n",
    "    3. For each fold, iterates through the different model types (e.g., logistic regression, random forest, dummy classifier).\n",
    "    4. For each model type, iterates through the different modalities (e.g., all features, demographic features, digital phenotyping features).\n",
    "    5. Calls `get_performance_single_model_subgroup()` to evaluate each model on the test data for the specified subgroup and retrieve performance metrics.\n",
    "    6. Concatenates the performance metrics of all models into a single DataFrame.\n",
    "    7. Sorts the DataFrame by modality and model type, and resets the index for clarity.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dir (str): The directory where the processed test data files are stored.\n",
    "    - model_dir (str): The directory where the trained model pickle files are stored.\n",
    "    - group_var (str): The variable used to define the subgroup (e.g., \"age_group\", \"edu_group\", \"sex_group\").\n",
    "    - group_number (int): The specific group number within the subgroup variable (e.g., 0 or 1).\n",
    "\n",
    "    Returns:\n",
    "    - df (DataFrame): A DataFrame containing the performance metrics for all evaluated models on the specified subgroup, sorted by modality and model type.\n",
    "    '''\n",
    "    \n",
    "    # Initialize an empty DataFrame to collect performance metrics from all models\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over the cross-validation folds (assumes 5-fold CV)\n",
    "    for fold in range(5):\n",
    "        # Iterate over each model type\n",
    "        for model_type in [\"dummy\", \"lr\", \"rf\"]:\n",
    "            # Iterate over each modality\n",
    "            for modality in [\"all\", \"demo\", \"dpd\"]:\n",
    "                # Evaluate the model on the specified subgroup and get performance metrics\n",
    "                performance = get_performance_single_model_subgroup(\n",
    "                    model_dir, model_type, modality, data_dir, fold, group_var, group_number\n",
    "                )\n",
    "                # Append the performance metrics to the DataFrame\n",
    "                df = pd.concat([df, performance], axis=0)\n",
    "    \n",
    "    # Sort the DataFrame by modality and model type, then reset the index\n",
    "    return df.sort_values(by=[\"Modality\", \"Model\"], ascending=True).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "def get_performance_subgroups(processed_dir, model_dir, group_var, group_number):\n",
    "    '''\n",
    "    This function evaluates the performance of multiple models across different folds, modalities, and model types for a specific subgroup.\n",
    "    It performs the following steps:\n",
    "\n",
    "    1. Calls `get_performance_multiple_models_subgroup()` to evaluate the performance of multiple models on the selected subgroup.\n",
    "    2. Groups the results by modality and model type, computes median values for performance metrics, and rounds them to 2 decimal places.\n",
    "    3. Resets the index, drops the \"Fold\" column, and saves the final output to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - processed_dir (str): The directory where the processed test data files are stored.\n",
    "    - model_dir (str): The directory where the trained model pickle files are stored.\n",
    "    - group_var (str): The variable used to define the subgroup (e.g., \"age_group\", \"edu_group\", \"sex_group\").\n",
    "    - group_number (int): The specific group number within the subgroup variable (e.g., 0 or 1).\n",
    "\n",
    "    Returns:\n",
    "    - None: The function saves the performance metrics for the subgroup to a CSV file.\n",
    "    '''\n",
    "    \n",
    "    # Evaluate multiple models on the selected subgroup and get performance metrics\n",
    "    df = get_performance_multiple_models_subgroup(processed_dir, model_dir, group_var, group_number)\n",
    "    \n",
    "    # Group the results by modality and model type, compute median values for performance metrics, and round them to 2 decimal places\n",
    "    df = (\n",
    "        df.groupby([\"Modality\", \"Model\"])\n",
    "        .aggregate([\"median\"])\n",
    "        .round(2)\n",
    "    )\n",
    "    \n",
    "    # Reset the index, drop the \"Fold\" column, and remove the level indicator in the column names\n",
    "    df = df.reset_index().drop([\"Fold\"], axis=1).droplevel(level=1, axis=1)\n",
    "    \n",
    "    # Save the performance metrics for the subgroup to a CSV file\n",
    "    df.iloc[[0, 1, 2, 7, 8, 4, 5], :].to_csv(performance_dir + \"model-performance-\" + group_var + \"-\" + str(group_number) + \".csv\", index=False)\n",
    "\n",
    "\n",
    "# Loop over each subgroup variable (age, education, sex) and each group within that variable, and evaluate the models\n",
    "for group_var in ['age_group', 'edu_group', 'sex_group']:\n",
    "    for group_number in [0, 1]:\n",
    "        get_performance_subgroups(processed_dir, model_dir, group_var, group_number)\n",
    "\n",
    "\n",
    "# Compare performance between subgroups for each variable (age, education, sex) by calculating the performance gap\n",
    "for group_var in ['age_group', 'edu_group', 'sex_group']:\n",
    "    # Load the performance metrics for the two groups within the subgroup variable\n",
    "    df_1 = pd.read_csv(performance_dir + 'model-performance-' + group_var + '-0.csv')\n",
    "    df_2 = pd.read_csv(performance_dir + 'model-performance-' + group_var + '-1.csv')\n",
    "    \n",
    "    # Compute the difference (gap) in performance between the two groups and save to a CSV file\n",
    "    (df_1.set_index(['Modality', 'Model']) - df_2.set_index(['Modality', 'Model'])).to_csv(performance_dir + 'performance-gap-' + group_var + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f070be-a3b3-4c66-963e-8f93f7b26ec5",
   "metadata": {},
   "source": [
    "## Model explanation\n",
    "\n",
    "We use the SHAP library to explain how our models make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "657b2361",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x190 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict_model(model, X_test):\n",
    "    '''\n",
    "    This function makes predictions on the test data using a specified model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (tuple): A tuple where the second element is the trained model (from GridSearchCV).\n",
    "    - X_test (DataFrame): The test data features.\n",
    "    \n",
    "    Returns:\n",
    "    - ndarray: The predicted labels from the model.\n",
    "    '''\n",
    "    return model[1].predict(X_test)\n",
    "\n",
    "\n",
    "def load_data_and_model(data_dir, model_dir, model_type, modality, fold):\n",
    "    '''\n",
    "    This function loads the test data and a pre-trained model for a specific fold, modality, and model type.\n",
    "    It also formats the feature names for interpretability in SHAP plots.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dir (str): The directory where the processed test data files are stored.\n",
    "    - model_dir (str): The directory where the trained model pickle files are stored.\n",
    "    - model_type (str): The type of model used (e.g., \"lr\", \"rf\").\n",
    "    - modality (str): The modality or feature set used for prediction (e.g., \"all\", \"demo\", \"dpd\").\n",
    "    - fold (int): The fold number corresponding to the cross-validation split.\n",
    "\n",
    "    Returns:\n",
    "    - X_test (DataFrame): The test data features, with formatted column names.\n",
    "    - model (object): The best estimator from the trained model.\n",
    "    '''\n",
    "    \n",
    "    # Load the test data based on the specified modality\n",
    "    if modality == \"all\":\n",
    "        X_test = pd.read_csv(data_dir + \"X_train_\" + str(fold) + \".csv\")\n",
    "    elif modality == \"demo\":\n",
    "        X_test = pd.read_csv(data_dir + \"X_train_\" + str(fold) + \".csv\").filter(like=modality)\n",
    "    else:\n",
    "        X_test = pd.read_csv(data_dir + \"X_train_\" + str(fold) + \".csv\")\n",
    "        demo_var_names = X_test.filter(like=\"demo\").columns\n",
    "        X_test = X_test[X_test.columns[~X_test.columns.isin(demo_var_names)]]\n",
    "\n",
    "    # Load the trained model\n",
    "    model = load_model(model_dir + model_type + \"_\" + modality + \"_\" + str(fold) + \".pkl\")\n",
    "\n",
    "    # Format the feature names for interpretability\n",
    "    X_test.columns = (\n",
    "        X_test.columns.str.replace(\"_\", \" \")\n",
    "        .str.replace(\"location \", \"\")\n",
    "        .str.replace(\"app \", \"\")\n",
    "        .str.replace(\"demo \", \"\")\n",
    "        .str.capitalize()\n",
    "    )\n",
    "\n",
    "    return X_test, model[1].best_estimator_\n",
    "\n",
    "\n",
    "def build_explainer(model_type, model, X_test):\n",
    "    '''\n",
    "    This function builds a SHAP explainer based on the model type and test data.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_type (str): The type of model used (e.g., \"lr\", \"rf\").\n",
    "    - model (object): The trained model for which explanations are being generated.\n",
    "    - X_test (DataFrame): The test data features used for generating SHAP values.\n",
    "    \n",
    "    Returns:\n",
    "    - explainer (shap.Explainer): A SHAP explainer object for the given model and data.\n",
    "    '''\n",
    "    \n",
    "    if model_type == \"lr\":\n",
    "        explainer = shap.LinearExplainer(model, X_test)\n",
    "    elif model_type == \"rf\":\n",
    "        explainer = shap.TreeExplainer(model, X_test)\n",
    "\n",
    "    return explainer\n",
    "\n",
    "\n",
    "def make_beeswarm_plot(model_type, explainer, X_test):\n",
    "    '''\n",
    "    This function generates a SHAP beeswarm plot to visualize feature importance.\n",
    "\n",
    "    Parameters:\n",
    "    - model_type (str): The type of model used (e.g., \"lr\", \"rf\").\n",
    "    - explainer (shap.Explainer): The SHAP explainer object for the model.\n",
    "    - X_test (DataFrame): The test data features used for generating SHAP values.\n",
    "    \n",
    "    Returns:\n",
    "    - None: The function generates and displays a SHAP beeswarm plot.\n",
    "    '''\n",
    "    \n",
    "    if model_type == \"lr\":\n",
    "        shap_values = explainer(X_test)\n",
    "        shap.plots.beeswarm(shap_values)\n",
    "    elif model_type == \"rf\":\n",
    "        shap_values = explainer(X_test, check_additivity=False)\n",
    "        shap.plots.beeswarm(shap_values[:, :, 1])\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Modify the \"Purples\" colormap for use in SHAP plots\n",
    "original_cmap = plt.cm.get_cmap(\"Purples\")\n",
    "colors = original_cmap(np.linspace(0.4, 1, 256))\n",
    "new_cmap = LinearSegmentedColormap.from_list(\"ModifiedPurples\", colors)\n",
    "\n",
    "\n",
    "def save_beeswarm_plot(\n",
    "    data_dir, model_dir, figure_dir, model_type, explainer, X_test, modality, fold\n",
    "):\n",
    "    '''\n",
    "    This function generates and saves a SHAP beeswarm plot to a file.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dir (str): The directory where the processed test data files are stored.\n",
    "    - model_dir (str): The directory where the trained model pickle files are stored.\n",
    "    - figure_dir (str): The directory where the SHAP plots will be saved.\n",
    "    - model_type (str): The type of model used (e.g., \"lr\", \"rf\").\n",
    "    - explainer (shap.Explainer): The SHAP explainer object for the model.\n",
    "    - X_test (DataFrame): The test data features used for generating SHAP values.\n",
    "    - modality (str): The modality or feature set used for prediction (e.g., \"all\", \"demo\", \"dpd\").\n",
    "    - fold (int): The fold number corresponding to the cross-validation split.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function saves a SHAP beeswarm plot as a PNG file.\n",
    "    '''\n",
    "    \n",
    "    if model_type == \"lr\":\n",
    "        shap_values = explainer(X_test)\n",
    "        shap.plots.beeswarm(\n",
    "            shap_values, show=False, max_display=20, axis_color=\"black\", color=new_cmap\n",
    "        )\n",
    "    elif model_type == \"rf\":\n",
    "        shap_values = explainer(X_test, check_additivity=False)\n",
    "        shap.plots.beeswarm(\n",
    "            shap_values[:, :, 1],\n",
    "            show=False,\n",
    "            max_display=200,\n",
    "            axis_color=\"black\",\n",
    "            color=new_cmap,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        figure_dir\n",
    "        + \"beeswarm_\"\n",
    "        + model_type\n",
    "        + \"_\"\n",
    "        + modality\n",
    "        + \"_\"\n",
    "        + str(fold)\n",
    "        + \".png\"\n",
    "    )\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def make_beeswarm_plot(\n",
    "    processed_dir, model_dir, figure_dir, model_type, modality, fold\n",
    "):\n",
    "    '''\n",
    "    This function coordinates the loading of data and model, building of the SHAP explainer, \n",
    "    and saving of the SHAP beeswarm plot for a specific fold, modality, and model type.\n",
    "\n",
    "    Parameters:\n",
    "    - processed_dir (str): The directory where the processed test data files are stored.\n",
    "    - model_dir (str): The directory where the trained model pickle files are stored.\n",
    "    - figure_dir (str): The directory where the SHAP plots will be saved.\n",
    "    - model_type (str): The type of model used (e.g., \"lr\", \"rf\").\n",
    "    - modality (str): The modality or feature set used for prediction (e.g., \"all\", \"demo\", \"dpd\").\n",
    "    - fold (int): The fold number corresponding to the cross-validation split.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function saves a SHAP beeswarm plot as a PNG file.\n",
    "    '''\n",
    "    \n",
    "    X_test, model = load_data_and_model(\n",
    "        processed_dir, model_dir, model_type, modality, fold\n",
    "    )\n",
    "\n",
    "    explainer = build_explainer(model_type, model, X_test)\n",
    "\n",
    "    save_beeswarm_plot(\n",
    "        processed_dir,\n",
    "        model_dir,\n",
    "        figure_dir,\n",
    "        model_type,\n",
    "        explainer,\n",
    "        X_test,\n",
    "        modality,\n",
    "        fold,\n",
    "    )\n",
    "\n",
    "\n",
    "def explain_multiple_models(processed_dir, model_dir, figure_dir):\n",
    "    '''\n",
    "    This function generates and saves SHAP beeswarm plots for multiple models across different folds and modalities.\n",
    "    \n",
    "    Parameters:\n",
    "    - processed_dir (str): The directory where the processed test data files are stored.\n",
    "    - model_dir (str): The directory where the trained model pickle files are stored.\n",
    "    - figure_dir (str): The directory where the SHAP plots will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function saves multiple SHAP beeswarm plots as PNG files.\n",
    "    '''\n",
    "    \n",
    "    for model_type in [\"lr\", \"rf\"]:\n",
    "        for modality in [\"all\", \"demo\", \"dpd\"]:\n",
    "            for fold in range(5):\n",
    "                make_beeswarm_plot(\n",
    "                    processed_dir, model_dir, figure_dir, model_type, modality, fold\n",
    "                )\n",
    "\n",
    "\n",
    "# Run the function to generate and save SHAP beeswarm plots for all models\n",
    "explain_multiple_models(processed_dir, model_dir, figure_dir)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
